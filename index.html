<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <header>
      <button
        onclick="toggleTheme()"
        style="position: fixed; top: 1rem; right: 1rem; z-index: 1000"
      >
        Toggle Theme
      </button>
    </header>
    <main>
      <section>
        <h1>CHAPTER THREE</h1>
        <h1>INTERNETWORKING</h1>
        <blockquote>
          Nature seems ... to reach many of her ends by long circuitous
          routes.<br />
          &mdash; Rudolph Lotze
        </blockquote>

        <h2>Problem: Not All Networks are Directly Connected</h2>
        <p>
          As we have seen, there are many technologies that can be used to build
          last-mile links or to connect a modest number of nodes together, but
          how do we build networks of global scale? A single Ethernet can
          interconnect no morethan 1024 hosts; a point-to-point link connects
          only two. Wireless networks are limited by the range of their radios.
          To build a global network, we need a way to interconnect these
          different types of links and multi-access networks. The concept of
          interconnecting different types of networks to build a large, global
          network is the core idea of the Internet and is often referred to as
          <em><strong>internetworking</strong></em
          >.
        </p>
        <p>
          We can divide the internetworking problem up into a few subproblems.
          First of all, we need a way to interconnect links. Devices that
          interconnect links of the same type are often called switches, or
          sometimes Layer 2 (L2) switches. These devices are the first topic of
          this chapter. A particularly important class of L2 switches in use
          today are those used to interconnect Ethernet segments. These switches
          are also sometimes called bridges.
        </p>
        <p>
          The core job of a switch is to take packets that arrive on an input
          and forward (or switch) them to the right output so that they will
          reach their appropriate destination. There are a variety of ways that
          the switch can determine the “right” output for a packet, which can be
          broadly categorized as connectionless and connection-oriented
          approaches. These two approaches have both found important application
          areas over the years.
        </p>
        <p>
          Given the enormous diversity of network types, we also need a way to
          interconnect disparate networks and links (i.e., deal with
          heterogeneity). Devices that perform this task, once called gateways,
          are now mostly known as routers, or alternatively, Layer 3 (L3)
          switches. The protocol that was invented to deal with interconnection
          of disparate network types, the Internet Protocol (IP), is the topic
          of our second section. Once we interconnect a whole lot of links and
          networks with switches and routers, there are likely to be many
          different possible ways to get from one point to another. Finding a
          suitable path or route through a network is one of the fundamental
          problems of networking. Such paths should be efficient (e.g., no
          longer than necessary), loop free, and able to respond to the fact
          that networks are not static—nodes may fail or reboot, links may
          break, and new nodes or links may be added. Our third section looks at
          some of the algorithms and protocols that have been developed to
          address these issues.
        </p>
        <p>
          Once we understand the problems of switching and routing, we need some
          devices to perform those func tions. This chapter concludes with some
          discussion of the ways switches and routers are implemented. While
          many packet switches and routers are quite similar to a
          general-purpose computer, there are many situations where more
          specialized designs are used. This is particularly the case at the
          high end, where there seems to be a never-ending need for more
          switching capacity that can handle the ever-increasing traffic load in
          the Internet’s core.
        </p>
      </section>
      <section>
        <h2>3.1 Switching Basics</h2>
        <p>
          In the simplest terms, a switch is a mechanism that allows us to
          interconnect links to form a larger network. A switch is a
          multi-input, multi-output device that transfers packets from an input
          to one or more outputs. Thus, a switch adds the star topology (see
          Figure 3.1) to the set of possible network structures. A star topology
          has several attractive properties:
        </p>
        <ul>
          <li>
            Even though a switch has a fixed number of inputs and outputs, which
            limits the number of hosts that can be connected to a single switch,
            large networks can be built by interconnecting a number of switches.
          </li>
          <li>
            We can connect switches to each other and to hosts using
            point-to-point links, which typically means that we can build
            networks of large geographic scope.
          </li>
          <li>
            Adding a new host to the network by connecting it to a switch does
            not necessarily reduce the perfor mance of the network for other
            hosts already connected.
          </li>
        </ul>
        <figure>
          <img
            src="images/fig3-1.PNG"
            alt="A switch provides a star topology"
            class="styled-image"
          />
          <figcaption>
            Figure 3.1: A switch provides a star topology.
          </figcaption>
        </figure>

        <p>
          This last claim cannot be made for the shared-media networks discussed
          in the last chapter. For example, it is impossible for two hosts on
          the same 10-Mbps Ethernet segment to transmit continuously at 10 Mbps
          because they share the same transmission medium. Every host on a
          switched network has its own link to the switch, so it may be entirely
          possible for many hosts to transmit at the full link speed
          (bandwidth), provided that the switch is designed with enough
          aggregate capacity. Providing high aggregate throughput is one of the
          design goals for a switch; we return to this topic later. In general,
          switched networks are considered more scalable (i.e., more capable of
          growing to large numbers of nodes) than shared-media networks because
          of this ability to support many hosts at full speed.
        </p>

        <section class="info-box">
          <h2>Dense Wavelength Division Multiplexing</h2>
          <p>
            Our focus on packet-switched networks obscures the fact that,
            especially in wide-area networks, the underlying physical transport
            is all-optical: there are no packets. At this layer, commercially
            available DWDM (Dense Wavelength Division Multiplexing) equipment is
            able to transmit a large number of optical wavelengths (colors) down
            a single fiber. For example, one might send data on 100 or more
            different wavelengths, and each wavelength might carry as much as
            100 Gbps of data.
          </p>
          <p>
            Connecting these fibers is an optical device called a ROADM
            (Reconfigurable Optical Add/Drop Multiplexer). A collection of
            ROADMs (nodes) and fibers (links) form an optical transport network,
            where each ROADM is able to forward individual wavelengths along a
            multi-hop path, creating a logical end-to-end circuit. From the
            perspective of a packet-switched network that might be constructed
            on top of this optical transport, one wavelength—even if it crosses
            multiple ROADMs—appears to be a single point-to-point link between
            two switches, over which one might elect to run SONET or 100-Gbps
            Ethernet as the framing protocol. The reconfigurability feature of
            ROADMs means that it is possible to change these underlying
            end-to-end wavelengths, effectively creating a new topology at the
            packet-switching layer.
          </p>
        </section>

        <p>
          A switch is connected to a set of links and, for each of these links,
          runs the appropriate data link protocol to communicate with the node
          at the other end of the link. A switch’s primary job is to receive
          incoming packets on one of its links and to transmit them on some
          other link. This function is sometimes referred to as either switching
          or forwarding, and in terms of the Open Systems Interconnection (OSI)
          architecture, it is the main function of the network layer, otherwise
          known as Layer 2.
        </p>

        <p>
          The question, then, is how does the switch decide which output link to
          place each packet on? The general answer is that it looks at the
          header of the packet for an identifier that it uses to make the
          decision. The details of how it uses this identifier vary, but there
          are two common approaches. The first is the datagram or connectionless
          approach. The second is the virtual circuit or connection-oriented
          approach. A third approach, source routing, is less common than these
          other two, but it does have some useful applications.
        </p>

        <p>
          One thing that is common to all networks is that we need to have a way
          to identify the end nodes. Such identifiers are usually called
          addresses. We have already seen examples of addresses, such as the
          48-bit address used for Ethernet. The only requirement for Ethernet
          addresses is that no two nodes on a network have the same address.
          This is accomplished by making sure that all Ethernet cards are
          assigned a globally unique identifier. For the following discussion,
          we assume that each host has a globally unique address. Later on, we
          consider other useful properties that an address might have, but
          global uniqueness is adequate to get us started.
        </p>
        <p>
          Another assumption that we need to make is that there is some way to
          identify the input and output ports of each switch. There are at least
          two sensible ways to identify ports: One is to number each port, and
          the other is to identify the port by the name of the node (switch or
          host) to which it leads. For now, we use numbering of the ports.
        </p>
      </section>
      <section>
        <h2>3.1.1 Datagrams</h2>
        <p>
          The idea behind datagrams is incredibly simple: You just include in
          every packet enough information to enable any switch to decide how to
          get it to its destination. That is, every packet contains the complete
          desti nation address. Consider the example network illustrated in
          Figure 3.2, in which the hosts have addresses A, B, C, and so on. To
          decide how to forward a packet, a switch consults a forwarding table
          (sometimes called a routing table), an example of which is depicted in
          Table 3.1. This particular table shows the forwarding information that
          switch 2 needs to forward datagrams in the example network. It is
          pretty easy to figure out such a table when you have a complete map of
          a simple network like that depicted here; we could imagine a network
          operator configuring the tables statically. It is a lot harder to
          create the forwarding tables in large, complex networks with
          dynamically changing topologies and multiple paths between
          destinations. That harder problem is known as routing and is the topic
          of a later section. We can think of routing as a process that takes
          place in the background so that, when a data packet turns up, we will
          have the right information in the forwarding table to be able to
          forward, or switch, the packet.
        </p>

        <figure>
          <img
            src="images/fig3-2.PNG"
            alt="Datagram forwarding: an example network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.2: Datagram forwarding: an example network.
          </figcaption>
        </figure>

        <table border="1">
          <caption>
            Table 3.1: Forwarding Table for Switch 2.
          </caption>
          <thead>
            <tr>
              <th>Destination</th>
              <th>Port</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>A</td>
              <td>3</td>
            </tr>
            <tr>
              <td>B</td>
              <td>0</td>
            </tr>
            <tr>
              <td>C</td>
              <td>3</td>
            </tr>
            <tr>
              <td>D</td>
              <td>3</td>
            </tr>
            <tr>
              <td>E</td>
              <td>2</td>
            </tr>
            <tr>
              <td>F</td>
              <td>1</td>
            </tr>
            <tr>
              <td>G</td>
              <td>0</td>
            </tr>
            <tr>
              <td>H</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>

        <p>Datagram networks have the following characteristics:</p>
        <ul>
          <li>
            A host can send a packet anywhere at any time, since any packet that
            turns up at a switch can be immediately forwarded (assuming a
            correctly populated forwarding table). For this reason, datagram
            networks are often called connectionless; this contrasts with the
            connection-oriented networks de scribed below, in which some
            connection state needs to be established before the first data
            packet is sent.
          </li>
          <li>
            When a host sends a packet, it has no way of knowing if the network
            is capable of delivering it or if the destination host is even up
            and running.
          </li>
          <li>
            Each packet is forwarded independently of previous packets that
            might have been sent to the same destination. Thus, two successive
            packets from host A to host B may follow completely different paths
            (perhaps because of a change in the forwarding table at some switch
            in the network).
          </li>
          <li>
            A switch or link failure might not have any serious effect on
            communication if it is possible to find an alternate route around
            the failure and to update the forwarding table accordingly.
          </li>
        </ul>
        <p>
          This last fact is particularly important to the history of datagram
          networks. One of the important design goals of the Internet is
          robustness to failures, and history has shown it to be quite effective
          at meeting this goal. Since datagram-based networks are the dominant
          technology discussed in this book, we postpone illustrative examples
          for the following sections, and move on to the two main alternatives.
        </p>
      </section>
      <section>
        <h2>3.1.2 Virtual Circuit Switching</h2>
        <p>
          A second technique for packet switching uses the concept of a virtual
          circuit (VC). This approach, which is also referred to as a
          connection-oriented model, requires setting up a virtual connection
          from the source host to the destination host before any data is sent.
          To understand how this works, consider Figure 3.3, where host A again
          wants to send packets to host B. We can think of this as a two-stage
          process. The first stage is “connection setup.” The second is data
          transfer. We consider each in turn.
        </p>
        <p>
          In the connection setup phase, it is necessary to establish a
          “connection state” in each of the switches between the source and
          destination hosts. The connection state for a single connection
          consists of an entry in a “VC table” in each switch through which the
          connection passes. One entry in the VC table on a single switch
          contains:
        </p>
        <ul>
          <li>
            Avirtual circuit identifier (VCI) that uniquely identifies the
            connection at this switch and which will be carried inside the
            header of the packets that belong to this connection
          </li>
          <li>
            An incoming interface on which packets for this VC arrive at the
            switch
          </li>
          <li>
            An outgoing interface in which packets for this VC leave the switch
          </li>
          <li>
            A potentially different VCI that will be used for outgoing packets
          </li>
        </ul>

        <figure>
          <img
            src="images/fig3-3.PNG"
            alt="An example of a virtual circuit network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.3: An example of a virtual circuit network.
          </figcaption>
        </figure>

        <p>
          The semantics of one such entry is as follows: If a packet arrives on
          the designated incoming interface and that packet contains the
          designated VCI value in its header, then that packet should be sent
          out the specified outgoing interface with the specified outgoing VCI
          value having been first placed in its header.
        </p>
        <p>
          Note that the combination of the VCI of packets as they are received
          at the switch and the interface on which they are received uniquely
          identifies the virtual connection. There may of course be many virtual
          connections established in the switch at one time. Also, we observe
          that the incoming and outgoing VCI values are generally not the same.
          Thus, the VCI is not a globally significant identifier for the
          connection; rather, it has significance only on a given link (i.e., it
          has link-local scope).
        </p>
        <p>
          Whenever a new connection is created, we need to assign a new VCI for
          that connection on each link that the connection will traverse. We
          also need to ensure that the chosen VCI on a given link is not
          currently in use on that link by some existing connection.
        </p>
        <p>
          There are two broad approaches to establishing connection state. One
          is to have a network administrator configure the state, in which case
          the virtual circuit is “permanent.” Of course, it can also be deleted
          by the administrator, so a permanent virtual circuit (PVC) might best
          be thought of as a long-lived or administratively configured VC.
          Alternatively, a host can send messages into the network to cause the
          state to be established. This is referred to as signalling, and the
          resulting virtual circuits are said to be switched. The salient
          characteristic of a switched virtual circuit (SVC) is that a host may
          set up and delete such a VC dynamically without the involvement of a
          network administrator. Note that an SVC should more accurately be
          called a signalled VC, since it is the use of signalling (not
          switching) that distinguishes an SVC from a PVC.
        </p>
        <p>
          Let’s assume that a network administrator wants to manually create a
          new virtual connection from host A to host B. First, the administrator
          needs to identify a path through the network from A to B. In the
          example network of Figure 3.3, there is only one such path, but in
          general, this may not be the case. The administrator then picks a VCI
          value that is currently unused on each link for the connection. For
          the purposes of our example, let’s suppose that the VCI value 5 is
          chosen for the link from host A to switch 1, and that 11 is chosen for
          the link from switch 1 to switch 2. In that case, switch 1 needs to
          have an entry in its VC table configured as shown in Table 3.2.
        </p>
        <table>
          <caption>
            Table 3.2: Example Virtual Circuit Table Entry for Switch 1.
          </caption>
          <thead>
            <tr>
              <th>Incoming Interface</th>
              <th>Incoming VCI</th>
              <th>Outgoing Interface</th>
              <th>Outgoing VCI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>5</td>
              <td>1</td>
              <td>11</td>
            </tr>
          </tbody>
        </table>

        <p>
          Similarly, suppose that the VCI of 7 is chosen to identify this
          connection on the link from switch 2 to switch 3 and that a VCI of 4
          is chosen for the link from switch 3 to host B. In that case, switches
          2 and 3 need to be configured with VC table entries as shown in Table
          3.3 and Table 3.4, respectively. Note that the “outgoing” VCI value at
          one switch is the “incoming” VCI value at the next switch.
        </p>
        <table>
          <caption>
            Table 3.3: Virtual Circuit Table Entry at Switch 2.
          </caption>
          <thead>
            <tr>
              <th>Incoming Interface</th>
              <th>Incoming VCI</th>
              <th>Outgoing Interface</th>
              <th>Outgoing VCI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>3</td>
              <td>11</td>
              <td>2</td>
              <td>7</td>
            </tr>
          </tbody>
        </table>

        <table>
          <caption>
            Table 3.4: Virtual Circuit Table Entry at Switch 3.
          </caption>
          <thead>
            <tr>
              <th>Incoming Interface</th>
              <th>Incoming VCI</th>
              <th>Outgoing Interface</th>
              <th>Outgoing VCI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>7</td>
              <td>1</td>
              <td>4</td>
            </tr>
          </tbody>
        </table>

        <figure>
          <img
            src="images/fig3-4.PNG"
            alt="A packet is sent into a virtual circuit network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.4: A packet is sent into a virtual circuit network.
          </figcaption>
        </figure>

        <p>
          Once the VC tables have been set up, the data transfer phase can
          proceed, as illustrated in Figure 3.4. For any packet that it wants to
          send to host B, A puts the VCI value of 5 in the header of the packet
          and sends it to switch 1. Switch 1 receives any such packet on
          interface 2, and it uses the combination of the interface and the VCI
          in the packet header to find the appropriate VC table entry. As shown
          in Table 3.2, the table entry in this case tells switch 1 to forward
          the packet out of interface 1 and to put the VCI value 11 in the
          header when the packet is sent. Thus, the packet will arrive at switch
          2 on interface 3 bearing VCI 11. Switch 2 looks up interface 3 and VCI
          11 in its VC table (as shown in Table 3.3) and sends the packet on to
          switch 3 after updating the VCI value in the packet header
          appropriately, as shown in Figure 3.5. This process continues until it
          arrives at host B with the VCI value of 4 in the packet. To host B,
          this identifies the packet as having come from host A.
        </p>
        <p>
          In real networks of reasonable size, the burden of configuring VC
          tables correctly in a large number of switches would quickly become
          excessive using the above procedures. Thus, either a network
          management tool or some sort of signalling (or both) is almost always
          used, even when setting up “permanent” VCs. In the case of PVCs,
          signalling is initiated by the network administrator, while SVCs are
          usually set up using signalling by one of the hosts. We consider now
          how the same VC just described could be set up by signalling from the
          host.
        </p>
        <figure>
          <img
            src="images/fig3-5.PNG"
            alt="A packet makes its way through a virtual circuit network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.5: A packet makes its way through a virtual circuit
            network.
          </figcaption>
        </figure>
        <p>
          To start the signalling process, host A sends a setup message into the
          network—that is, to switch 1. The setup message contains, among other
          things, the complete destination address of host B. The setup message
          needs to get all the way to B to create the necessary connection state
          in every switch along the way. We can see that getting the setup
          message to B is a lot like getting a datagram to B, in that the
          switches have to know which output to send the setup message to so
          that it eventually reaches B. For now, let’s just assume that the
          switches know enough about the network topology to figure out how to
          do that, so that the setup message flows on to switches 2 and 3 before
          finally reaching host B.
        </p>
        <p>
          When switch 1 receives the connection request, in addition to sending
          it on to switch 2, it creates a new entry in its virtual circuit table
          for this new connection. This entry is exactly the same as shown
          previously in Table 3.2. The main difference is that now the task of
          assigning an unused VCI value on the interface is performed by the
          switch for that port. In this example, the switch picks the value 5.
          The virtual circuit table now has the following information: “When
          packets arrive on port 2 with identifier 5, send them out on port 1.”
          Another issue is that, somehow, host A will need to learn that it
          should put the VCI value of 5 in packets that it wants to send to B;
          we will see how that happens below.
        </p>
        <p>
          When switch 2 receives the setup message, it performs a similar
          process; in this example, it picks the value 11 as the incoming VCI
          value. Similarly, switch 3 picks 7 as the value for its incoming VCI.
          Each switch can pick any number it likes, as long as that number is
          not currently in use for some other connection on that port of that
          switch. As noted above, VCIs have link-local scope; that is, they have
          no global significance. Finally, the setup message arrives as host B.
          Assuming that B is healthy and willing to accept a connection from
          host A, it too allocates an incoming VCI value, in this case 4. This
          VCI value can be used by B to identify all packets coming from host A.
        </p>
        <p>
          Now, to complete the connection, everyone needs to be told what their
          downstream neighbor is using as the VCI for this connection. Host B
          sends an acknowledgment of the connection setup to switch 3 and
          includes in that message the VCI that it chose (4). Now switch 3 can
          complete the virtual circuit table entry for this connection, since it
          knows the outgoing value must be 4. Switch 3 sends the acknowledgment
          on to switch 2, specifying a VCI of 7. Switch 2 sends the message on
          to switch 1, specifying a VCI of 11. Finally, switch 1 passes the
          acknowledgment on to host A, telling it to use the VCI of 5 for this
          connection.
        </p>
        <p>
          At this point, everyone knows all that is necessary to allow traffic
          to flow from host A to host B. Each switch has a complete virtual
          circuit table entry for the connection. Furthermore, host A has a firm
          acknowledgment that everything is in place all the way to host B. At
          this point, the connection table entries are in place in all three
          switches just as in the administratively configured example above, but
          the whole process happened automatically in response to the signalling
          message sent from A. The data transfer phase can now begin and is
          identical to that used in the PVC case.
        </p>
        <p>
          When host A no longer wants to send data to host B, it tears down the
          connection by sending a teardown message to switch 1. The switch
          removes the relevant entry from its table and forwards the message on
          to the other switches in the path, which similarly delete the
          appropriate table entries. At this point, if host A were to send a
          packet with a VCI of 5 to switch 1, it would be dropped as if the
          connection had never existed.
        </p>

        <p>There are several things to note about virtual circuit switching:</p>
        <ul>
          <li>
            Since host A has to wait for the connection request to reach the far
            side of the network and return before it can send its first data
            packet, there is at least one round-trip time (RTT) of delay before
            data is sent.
          </li>
          <li>
            While the connection request contains the full address for host B
            (which might be quite large, being a global identifier on the
            network), each data packet contains only a small identifier, which
            is only unique on one link. Thus, the per-packet overhead caused by
            the header is reduced relative to the datagram model. More
            importantly, the lookup is fast because the virtual circuit number
            can be treated as an index into a table rather than as a key that
            has to be looked up.
          </li>
          <li>
            If a switch or a link in a connection fails, the connection is
            broken and a new one will need to be established. Also, the old one
            needs to be torn down to free up table storage space in the
            switches.
          </li>
          <li>
            The issue of how a switch decides which link to forward the
            connection request on has been glossed over. In essence, this is the
            same problem as building up the forwarding table for datagram
            forward ing, which requires some sort of routing algorithm. Routing
            is described in a later section, and the algorithms described there
            are generally applicable to routing setup requests as well as
            datagrams.
          </li>
        </ul>

        <p>
          One of the nice aspects of virtual circuits is that by the time the
          host gets the go-ahead to send data, it knows quite a lot about the
          network—for example, that there really is a route to the receiver and
          that the receiver is willing and able to receive data. It is also
          possible to allocate resources to the virtual circuit at the time it
          is established. For example, X.25 (an early and now largely obsolete
          virtual-circuit-based networking technology) employed the following
          three-part strategy:
        </p>
        <ol>
          <li>
            Buffers are allocated to each virtual circuit when the circuit is
            initialized.
          </li>
          <li>
            The sliding window protocol is run between each pair of nodes along
            the virtual circuit, and this protocol is augmented with flow
            control to keep the sending node from over-running the buffers
            allocated at the receiving node.
          </li>
          <li>
            The circuit is rejected by a given node if not enough buffers are
            available at that node when the connection request message is
            processed.
          </li>
        </ol>
        <p>
          In doing these three things, each node is ensured of having the
          buffers it needs to queue the packets that arrive on that circuit.
          This basic strategy is usually called hop-by-hop flow control.
        </p>
        <p>
          By comparison, a datagram network has no connection establishment
          phase, and each switch processes each packet independently, making it
          less obvious how a datagram network would allocate resources in a
          meaningful way. Instead, each arriving packet competes with all other
          packets for buffer space. If there are no free buffers, the incoming
          packet must be discarded. We observe, however, that even in a
          datagram-based network a source host often sends a sequence of packets
          to the same destination host. It is possible for each switch to
          distinguish among the set of packets it currently has queued, based on
          the source/destination pair, and thus for the switch to ensure that
          the packets belonging to each source/destination pair are receiving a
          fair share of the switch’s buffers.
        </p>
        <p>
          In the virtual circuit model, we could imagine providing each circuit
          with a different quality of service (QoS). In this setting, the term
          quality of service is usually taken to mean that the network gives the
          user some kind of performance-related guarantee, which in turn implies
          that switches set aside the resources they need to meet this
          guarantee. For example, the switches along a given virtual circuit
          might allocate a percentage of each outgoing link’s bandwidth to that
          circuit. As another example, a sequence of switches might ensure that
          packets belonging to a particular circuit not be delayed (queued) for
          more than a certain amount of time.
        </p>
        <p>
          There have been a number of successful examples of virtual circuit
          technologies over the years, notably X.25, Frame Relay, and
          Asynchronous Transfer Mode (ATM). With the success of the Internet’s
          connec tionless model, however, none of them enjoys great popularity
          today. One of the most common applications of virtual circuits for
          many years was the construction of virtual private networks (VPNs), a
          subject dis cussed in a later section. Even that application is now
          mostly supported using Internet-based technologies today.
        </p>
      </section>
      <section>
        <h2>Asynchronous Transfer Mode (ATM)</h2>
        <p>
          Asynchronous Transfer Mode (ATM) is probably the most well-known
          virtual circuit-based networking technology, although it is now well
          past its peak in terms of deployment. ATM became an important tech
          nology in the 1980s and early 1990s for a variety of reasons, not the
          least of which is that it was embraced by the telephone industry,
          which at that point in time was less active in computer networks
          (other than as a supplier of links from which other people built
          networks). ATM also happened to be in the right place at the right
          time, as a high-speed switching technology that appeared on the scene
          just when shared media like Ethernet and token rings were starting to
          look a bit too slow for many users of computer networks. In some ways
          ATM was a competing technology with Ethernet switching, and it was
          seen by many as a competitor to IP as well.
        </p>
        <figure>
          <img
            src="images/fig3-6.PNG"
            alt="ATM cell format at the UNI"
            class="styled-image"
          />
          <figcaption>Figure 3.6: ATM cell format at the UNI.</figcaption>
        </figure>

        <p>
          The approach ATM takes has some interesting properties, which makes it
          worth examining a bit further. The picture of the ATM packet
          format—more commonly called an ATM cell—in Figure 3.6 will illustrate
          the main points. We’ll skip the generic flow control (GFC) bits, which
          never saw much use, and start with the 24 bits that are labelled VPI
          (virtual path identifier—8 bits) and VCI (virtual circuit
          identifier—16 bits). If you consider these bits together as a single
          24-bit field, they correspond to the virtual circuit identifier
          introduced above. The reason for breaking the field into two parts was
          to allow for a level of hierarchy: All the circuits with the same VPI
          could, in some cases, be treated as a group (a virtual path) and could
          all be switched together looking only at the VPI, simplifying the work
          of a switch that could ignore all the VCI bits and reducing the size
          of the VC table considerably.
        </p>
        <p>
          Skipping to the last header byte we find an 8-bit cyclic redundancy
          check (CRC), known as the header error check (HEC). It uses CRC-8 and
          provides error detection and single-bit error correction capability on
          the cell header only. Protecting the cell header is particularly
          important because an error in the VCI will cause the cell to be
          misdelivered.
        </p>
        <p>
          Probably the most significant thing to notice about the ATM cell, and
          the reason it is called a cell and not a packet, is that it comes in
          only one size: 53 bytes. What was the reason for this? One big reason
          was to facilitate the implementation of hardware switches. When ATM
          was being created in the mid- and late 1980s, 10-Mbps Ethernet was the
          cutting-edge technology in terms of speed. To go much faster, most
          people thought in terms of hardware. Also, in the telephone world,
          people think big when they think of switches—telephone switches often
          serve tens of thousands of customers. Fixed-length packets turn out to
          be a very helpful thing if you want to build fast, highly scalable
          switches. There are two main reasons for this:
        </p>
        <ol>
          <li>
            It is easier to build hardware to do simple jobs, and the job of
            processing packets is simpler when you already know how long each
            one will be.
          </li>
          <li>
            If all packets are the same length, then you can have lots of
            switching elements all doing much the same thing in parallel, each
            of them taking the same time to do its job.
          </li>
        </ol>
        <p>
          This second reason, the enabling of parallelism, greatly improves the
          scalability of switch designs. It would be overstating the case to say
          that fast parallel hardware switches can only be built using
          fixed-length cells. However, it is certainly true that cells ease the
          task of building such hardware and that there was a lot of knowledge
          available about how to build cell switches in hardware at the time the
          ATM standards were being defined. As it turns out, this same principle
          is still applied in many switches and routers today, even if they deal
          in variable length packets—they cut those packets into some sort of
          cell in order to forward them from input port to output port, but this
          is all internal to the switch.
        </p>
        <p>
          There is another good argument in favor of small ATM cells, having to
          do with end-to-end latency. ATM was designed to carry both voice phone
          calls (the dominant use case at the time) and data. Because voice is
          low-bandwidth but has strict delay requirements, the last thing you
          want is for a small voice packet queued behind a large data packet at
          a switch. If you force all packets to be small (i.e., cell-sized),
          then large data packets can still be supported by reassembling a set
          of cells into a packet, and you get the benefit of being able to
          interleave the forwarding of voice cells and data cells at every
          switch along the path from source to destination. This idea of using
          small cells to improve end-to-end latency is alive and well today in
          cellular access networks.
        </p>
        <p>
          Having decided to use small, fixed-length packets, the next question
          was what is the right length to fix them at? If you make them too
          short, then the amount of header information that needs to be carried
          around relative to the amount of data that fits in one cell gets
          larger, so the percentage of link bandwidth that is actually used to
          carry data goes down. Even more seriously, if you build a device that
          processes cells at some maximum number of cells per second, then as
          cells get shorter the total data rate drops in direct proportion to
          cell size. An example of such a device might be a network adaptor that
          reassembles cells into larger units before handing them up to the
          host. The performance of such a device depends directly on cell size.
          On the other hand, if you make the cells too big, then there is a
          problem of wasted bandwidth caused by the need to pad transmitted data
          to fill a complete cell. If the cell payload size is 48 bytes and you
          want to send 1 byte, you’ll need to send 47 bytes of padding. If this
          happens a lot, then the utilization of the link will be very low. The
          combination of relatively high header-to-payload ratio plus the
          frequency of sending partially filled cells did actually lead to some
          noticeable inefficiency in ATM networks that some detractors called
          the cell tax.
        </p>
        As it turns out, 48 bytes was picked for the ATM cell payload as a
        compromise. There were good arguments for both larger and smaller cells,
        and 48 made almost no one happy—a power of two would certainly have been
        better for computers to process.
      </section>
      <section>
        <h2>3.1.3 Source Routing</h2>
        <p>
          A third approach to switching that uses neither virtual circuits nor
          conventional datagrams is known as source routing. The name derives
          from the fact that all the information about network topology that is
          required to switch a packet across the network is provided by the
          source host.
        </p>
        <p>
          There are various ways to implement source routing. One would be to
          assign a number to each output of each switch and to place that number
          in the header of the packet. The switching function is then very
          simple: For each packet that arrives on an input, the switch would
          read the port number in the header and transmit the packet on that
          output. However, since there will in general be more than one switch
          in the path between the sending and the receiving host, the header for
          the packet needs to contain enough information to allow every switch
          in the path to determine which output the packet needs to be placed
          on. One way to do this would be to put an ordered list of switch ports
          in the header and to rotate the list so that the next switch in the
          path is always at the front of the list. Figure 3.7 illustrates this
          idea.
        </p>
        <figure>
          <img
            src="images/fig3-7.PNG"
            alt="Source routing in a switched network (where the switch reads the rightmost number)"
            class="styled-image"
          />
          <figcaption>
            Figure 3.7: Source routing in a switched network (where the switch
            reads the rightmost number).
          </figcaption>
        </figure>

        <p>
          In this example, the packet needs to traverse three switches to get
          from host A to host B. At switch 1, it needs to exit on port 1, at the
          next switch it needs to exit at port 0, and at the third switch it
          needs to exit at port 3. Thus, the original header when the packet
          leaves host A contains the list of ports (3, 0, 1), where we assume
          that each switch reads the rightmost element of the list. To make sure
          that the next switch gets the appropriate information, each switch
          rotates the list after it has read its own entry. Thus, the packet
          header as it leaves switch 1 enroute to switch 2 is now (1, 3, 0);
          switch 2 performs another rotation and sends out a packet with (0, 1,
          3) in the header. Although not shown, switch 3 performs yet another
          rotation, restoring the header to what it was when host A sent it.
        </p>
        <p>
          There are several things to note about this approach. First, it
          assumes that host A knows enough about the topology of the network to
          form a header that has all the right directions in it for every switch
          in the path. This is somewhat analogous to the problem of building the
          forwarding tables in a datagram network or figuring out where to send
          a setup packet in a virtual circuit network. In practice, however, it
          is the first switch at the ingress to the network (as opposed to the
          end host connected to that switch) that appends the source route.
        </p>
        <p>
          Second, observe that we cannot predict how big the header needs to be,
          since it must be able to hold one word of information for every switch
          on the path. This implies that headers are probably of variable length
          with no upper bound, unless we can predict with absolute certainty the
          maximum number of switches through which a packet will ever need to
          pass.
        </p>
        <p>
          Third, there are some variations on this approach. For example, rather
          than rotate the header, each switch could just strip the first element
          as it uses it. Rotation has an advantage over stripping, however: Host
          B gets a copy of the complete header, which may help it figure out how
          to get back to host A. Yet another alternative is to have the header
          carry a pointer to the current “next port” entry, so that each switch
          just updates the pointer rather than rotating the header; this may be
          more efficient to implement. We show these three approaches in Figure
          3.8. In each case, the entry that this switch needs to read is A, and
          the entry that the next switch needs to read is B.
        </p>
        <figure>
          <img
            src="images/fig3-8.PNG"
            alt="Three ways to handle headers for source routing: rotation, stripping, pointer. Labels are read right to left."
            class="styled-image"
          />
          <figcaption>
            Figure 3.8: Three ways to handle headers for source routing: (a)
            rotation; (b) stripping; (c) pointer. The labels are read right to
            left.
          </figcaption>
        </figure>
        <p>
          Source routing can be used in both datagram networks and virtual
          circuit networks. For example, the Internet Protocol, which is a
          datagram protocol, includes a source route option that allows selected
          packets to be source routed, while the majority are switched as
          conventional datagrams. Source routing is also used in some virtual
          circuit networks as the means to get the initial setup request along
          the path from source to destination.
        </p>
        <p>
          Source routes are sometimes categorized as strict or loose. In a
          strict source route, every node along the path must be specified,
          whereas a loose source route only specifies a set of nodes to be
          traversed, without saying exactly how to get from one node to the
          next. A loose source route can be thought of as a set of waypoints
          rather than a completely specified route. The loose option can be
          helpful to limit the amount of information that a source must obtain
          to create a source route. In any reasonably large network, it is
          likely to be hard for a host to get the complete path information it
          needs to construct correct a strict source route to any destination.
          But both types of source routes do find application in certain
          scenarios, as we will see in later chapters.
        </p>
      </section>
      <section>
        <h2>3.2 Switched Ethernet</h2>
        <p>
          Having discussed some of the basic ideas behind switching, we now
          focus more closely on a specific switch ing technology: Switched
          Ethernet. The switches used to build such networks, which are often
          referred to as L2 switches, are widely used in campus and enterprise
          networks. Historically, they were more commonly referred to as bridges
          because they were used to “bridge” ethernet segments to build an
          extended LAN. But today most networks deploy Ethernet in a
          point-to-point configuration, with these links interconneted by L2
          switches to form a switched Ethernet.
        </p>
        <p>
          Thefollowing starts with the historical perspective (using bridges to
          connect a set of Ethernet segments), and then shifts to the
          perspective in wide-spread use today (using L2 switches to connect a
          set of point-to-point links). But whether we call the device a bridge
          or a switch—and the network you build an extended LAN or a switched
          Ethernet—the two behave in exactly the same way.
        </p>
        <p>
          To begin, suppose you have a pair of Ethernets that you want to
          interconnect. One approach you might try is to put a repeater between
          them. This would not be a workable solution, however, if doing so
          exceeded the physical limitations of the Ethernet. (Recall that no
          more than two repeaters between any pair of hosts and no more than a
          total of 2500 m in length are allowed.) An alternative would be to put
          a node with a pair of Ethernet adaptors between the two Ethernets and
          have the node forward frames from one Ethernet to the other. This node
          would differ from a repeater, which operates on bits, not frames, and
          just blindly copies the bits received on one interface to another.
          Instead, this node would fully implement the Ethernet’s collision
          detection and media access protocols on each interface. Hence, the
          length and number-of-host restrictions of the Ethernet, which are all
          about managing collisions, would not apply to the combined pair of
          Ethernets connected in this way. This device operates in promiscuous
          mode, accepting all frames transmitted on either of the Ethernets, and
          forwarding them to the other.
        </p>
        <p>
          In their simplest variants, bridges simply accept LAN frames on their
          inputs and forward them out on all other outputs. This simple strategy
          was used by early bridges but has some pretty serious limitations as
          we’ll see below. A number of refinements were added over the years to
          make bridges an effective mechanism for interconnecting a set of LANs.
          The rest of this section fills in the more interesting details.
        </p>
      </section>
      <section>
        <h2>3.2.1 Learning Bridges</h2>
        <p>
          The first optimization we can make to a bridge is to observe that it
          need not forward all frames that it receives. Consider the bridge in
          Figure 3.9. Whenever a frame from host A that is addressed to host B
          arrives on port 1, there is no need for the bridge to forward the
          frame out over port 2. The question, then, is how does a bridge come
          to learn on which port the various hosts reside?
        </p>
        <p>
          One option would be to have a human download a table into the bridge
          similar to the one given in Table 3.5. Then, whenever the bridge
          receives a frame on port 1 that is addressed to host A, it would not
          forward the frame out on port 2; there would be no need because host A
          would have already directly received the frame on the LAN connected to
          port 1. Anytime a frame addressed to host A was received on port 2,
          the bridge would forward the frame out on port 1.
        </p>
        <figure>
          <img
            src="images/fig3-9.PNG"
            alt="Illustration of a learning bridge"
            class="styled-image"
          />
          <figcaption>
            Figure 3.9: Illustration of a learning bridge.
          </figcaption>
        </figure>
        <figure>
          <figcaption>
            Table 3.5: Forwarding Table Maintained by a Bridge.
          </figcaption>
          <table>
            <thead>
              <tr>
                <th>Host</th>
                <th>Port</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>A</td>
                <td>1</td>
              </tr>
              <tr>
                <td>B</td>
                <td>1</td>
              </tr>
              <tr>
                <td>C</td>
                <td>1</td>
              </tr>
              <tr>
                <td>X</td>
                <td>2</td>
              </tr>
              <tr>
                <td>Y</td>
                <td>2</td>
              </tr>
              <tr>
                <td>Z</td>
                <td>2</td>
              </tr>
            </tbody>
          </table>
        </figure>

        <p>
          Having a human maintain this table is too burdensome, and there is a
          simple trick by which a bridge can learn this information for itself.
          The idea is for each bridge to inspect the source address in all the
          frames it receives. Thus, when host A sends a frame to a host on
          either side of the bridge, the bridge receives this frame and records
          the fact that a frame from host A was just received on port 1. In this
          way, the bridge can build a table just like Table 3.5.
        </p>
        <p>
          Note that a bridge using such a table implements a version of the
          datagram (or connectionless) model of forwarding described earlier.
          Each packet carries a global address, and the bridge decides which
          output to send a packet on by looking up that address in a table.
        </p>
        <p>
          Whenabridge first boots, this table is empty; entries are added over
          time. Also, a timeout is associated with each entry, and the bridge
          discards the entry after a specified period of time. This is to
          protect against the situation in which a host—and, as a consequence,
          its LAN address—is moved from one network to another. Thus, this table
          is not necessarily complete. Should the bridge receive a frame that is
          addressed to a host not currently in the table, it goes ahead and
          forwards the frame out on all the other ports. In other words, this
          table is simply an optimization that filters out some frames; it is
          not required for correctness.
        </p>
      </section>
      <section>
        <h2>3.2.2 Implementation</h2>
        <p>
          The code that implements the learning bridge algorithm is quite
          simple, and we sketch it here. Structure
          <strong>BridgeEntry</strong> defines a single entry in the bridge’s
          forwarding table; these are stored in a <strong>Map</strong> structure
          (which supports <strong>mapCreate</strong>, <strong>mapBind</strong>,
          and <strong>mapResolve</strong> operations) to enable entries to be
          efficiently located when packets arrive from sources already in the
          table.The constant <strong>MAX_TTL</strong> specifies how long an
          entry is kept in the table before it is discarded.
        </p>

        <figure>
          <img
            src="images/code-img1.PNG"
            alt="Code example image"
            class="styled-image"
          />
          <figcaption>
            Figure: Code sketch of the learning bridge algorithm, showing
            BridgeEntry structure and use of a Map for efficient forwarding
            table management.
          </figcaption>
        </figure>
        <p>
          The routine that updates the forwarding table when a new packet
          arrives is given by <strong>updateTable</strong>.The arguments passed
          are the source media access control <strong>(MAC)</strong> address
          contained in the packet and the interface number on which it was
          received.Another routine, not shown here, is invoked at regular
          intervals, scans the entries in the forwarding table, and decrements
          the <strong>TTL (time to live)</strong> field of eachentry, discarding
          any entries whose <strong>TTL</strong> has reached 0. Note that the
          <strong>TTL</strong> is reset to <strong>MAX_TTL</strong> every time a
          packet arrives to refresh an existing table entry and that the
          interface on which the destination can be reached is updated to
          reflect the most recently received packet.
        </p>
        <figure>
          <img
            src="images/code-img2.PNG"
            alt="Forwarding table update routine"
            class="styled-image"
          />
          <figcaption>
            Figure: The <code>updateTable</code> routine updates the forwarding
            table using the source MAC address and interface number. TTL values
            are refreshed with each packet and expired entries are periodically
            removed.
          </figcaption>
        </figure>
        <p>
          Note that this implementation adopts a simple strategy in the case
          where the bridge table has become full to capacity—it simply fails to
          add the new address. Recall that completeness of the bridge table is
          not necessary for correct forwarding; it just optimizes performance.
          If there is some entry in the table that is not currently being used,
          it will eventually time out and be removed, creating space for a new
          entry. An alternative approach would be to invoke some sort of cache
          replacement algorithm on finding the table full; for example, we might
          locate and remove the entry with the smallest TTL to accommodate the
          new entry.
        </p>
      </section>
      <section>
        <h2>3.2.3 Spanning Tree Algorithm</h2>
        <p>
          The preceding strategy works just fine until the network has a loop in
          it, in which case it fails in a horrible way—frames potentially get
          forwarded forever. This is easy to see in the example depicted in
          Figure 3.10, where switches S1, S4, and S6 form a loop.
        </p>
        <figure>
          <img
            src="images/fig3-10.PNG"
            alt="Switched Ethernet with loops"
            class="styled-image"
          />
          <figcaption>Figure 3.10: Switched Ethernet with loops.</figcaption>
        </figure>
        <p>
          Note that we are now making the shift from calling the each forwarding
          device a bridge (connecting seg ments that might reach multiple other
          devices) to instead calling them L2 switches (connecting point-to
          point links that reach just one other device). To keep the example
          managable, we include just three hosts. In practice, switches
          typically have 16, 24, or 48 ports, meaning they are able to connect
          to that many hosts (and other swiches).
        </p>
        <p>
          In our example switched network, suppose that a packet enters switch
          S4 from Host C and that the desti nation address is one not yet in any
          switches’s forwarding table: S4 sends a copy of the packet out its two
          other ports: to switches S1 and S6. Switch S6 forwards the packet onto
          S1 (and meanwhile, S1 forwards the packet onto S6), both of which in
          turn forward their packets back to S4. Switch S4 still doesn’t have
          this destination in its table, so it forwards the packet out its two
          other ports. There is nothing to stop this cycle from repeating
          endlessly, with packets looping in both directions among S1, S4, and
          S6.
        </p>
        <p>
          Why would a switched Ethernet (or extended LAN) come to have a loop in
          it? One possibility is that the network is managed by more than one
          administrator, for example, because it spans multiple departments in
          an organization. In such a setting, it is possible that no single
          person knows the entire configuration of the network, meaning that a
          switch that closes a loop might be added without anyone knowing. A
          second, more likely scenario is that loops are built into the network
          on purpose—to provide redundancy in case of failure. After all, a
          network with no loops needs only one link failure to become split into
          two separate partitions.
        </p>
        <p>
          Whatever the cause, switches must be able to correctly handle loops.
          This problem is addressed by having the switches run a distributed
          spanning tree algorithm. If you think of the network as being
          represented by a graph that possibly has loops (cycles), then a
          spanning tree is a subgraph of this graph that covers (spans) all the
          vertices but contains no cycles. That is, a spanning tree keeps all of
          the vertices of the original graph but throws out some of the edges.
          For example, Figure 3.11 shows a cyclic graph on the left and one of
          possibly many spanning trees on the right.
        </p>
        <figure>
          <img
            src="images/fig3-11.PNG"
            alt="Cyclic graph and corresponding spanning tree"
            class="styled-image"
          />
          <figcaption>
            Figure 3.11: Example of (a) a cyclic graph; (b) a corresponding
            spanning tree.
          </figcaption>
        </figure>

        <p>
          The idea of a spanning tree is simple enough: It’s a subset of the
          actual network topology that has no loops and that reaches all the
          devices in the network. The hard part is how all of the switches
          coordinate their decisions to arrive at a single view of the spanning
          tree. After all, one topology is typically able to be covered by
          multiple spanning trees. The answer lies in the spanning tree
          protocol, which we’ll describe now.
        </p>
        <p>
          The spanning tree algorithm, which was developed by Radia Perlman,
          then at the Digital Equipment Cor poration, is a protocol used by a
          set of switches to agree upon a spanning tree for a particular
          network. (The IEEE 802.1 specification is based on this algorithm.) In
          practice, this means that each switch decides the ports over which it
          is and is not willing to forward frames. In a sense, it is by removing
          ports from the topology that the network is reduced to an acyclic
          tree. It is even possible that an entire switch will not participate
          in forwarding frames, which seems kind of strange at first glance. The
          algorithm is dynamic, however, meaning that the switches are always
          prepared to reconfigure themselves into a new spanning tree should
          some switch fail, and so those unused ports and switches provide the
          redundant capacity needed to recover from failures.
        </p>

        <p>
          The mainidea of the spanning tree is for the switches to select the
          ports over which they will forward frames. The algorithm selects ports
          as follows. Each switch has a unique identifier; for our purposes, we
          use the labels S1, S2, S3, and so on. The algorithm first elects the
          switch with the smallest ID as the root of the spanning tree; exactly
          how this election takes place is described below. The root switch
          always forwards frames out over all of its ports. Next, each switch
          computes the shortest path to the root and notes which of its ports is
          on this path. This port is also selected as the switch’s preferred
          path to the root. Finally, to account for the possibility there could
          be another switch connected to its ports, the switch elect a single
          designated switch that will be responsible for forwarding frames
          toward the root. Each designated switch is the one that is closest to
          the root. If two or more switches are equally close to the root, then
          the switches’ identifiers are used to break ties, and the smallest ID
          wins. Of course, each switch might be connected to more than one other
          switch, so it participates in the election of a designated switch for
          each such port. In effect, this means that each switch decides if it
          is the designated switch relative to each of its ports. The switch
          forwards frames over those ports for which it is the designated
          switch.
        </p>

        <section aria-labelledby="fig3-12-context">
          <h3 id="fig3-12-context">
            Spanning Tree Root and Designated Switches
          </h3>

          <figure>
            <img
              src="images/fig3-12.PNG"
              alt="Spanning tree with some ports not selected"
              class="styled-image"
            />
            <figcaption>
              Figure 3.12: Spanning tree with some ports not selected.
            </figcaption>
          </figure>

          <p>
            Figure 3.12 shows the spanning tree that corresponds to the network
            shown in Figure 3.10. In this example, S1 is the root, since it has
            the smallest ID. Notice that S3 and S5 are connected to each other,
            but S5 is the designated switch since it is closer to the root.
            Similarly, S5 and S7 are connected to each other, but in this case
            S5 is the designated switch since it has the smaller ID; both are an
            equal distance from S1.
          </p>
        </section>

        <p>
          While it is possible for a human to look at the network given in
          Figure 3.10 and to compute the spanning tree given in the Figure 3.12
          according to the rules given above, the switches do not have the
          luxury of being able to see the topology of the entire network, let
          alone peek inside other switches to see their ID. Instead, they have
          to exchange configuration messages with each other and then decide
          whether or not they are the root or a designated switch based on these
          messages.
        </p>
        <p>
          Specifically, the configuration messages contain three pieces of
          information:
        </p>
        <ol>
          <li>The ID for the switch that is sending the message.</li>
          <li>
            The ID for what the sending switch believes to be the root switch.
          </li>
          <li>
            The distance, measured in hops, from the sending switch to the root
            switch.
          </li>
        </ol>
        <p>
          Each switch records the current best configuration message it has seen
          on each of its ports (“best” is defined below), including both
          messages it has received from other switches and messages that it has
          itself transmitted.
        </p>
        <p>
          Initially, each switch thinks it is the root, and so it sends a
          configuration message out on each of its ports identifying itself as
          the root and giving a distance to the root of 0. Upon receiving a
          configuration message over a particular port, the switch checks to see
          if that new message is better than the current best configura tion
          message recorded for that port. The new configuration message is
          considered better than the currently recorded information if any of
          the following is true:
        </p>
        <ul>
          <li>It identifies a root with a smaller ID.</li>
          <li>
            It identifies a root with an equal ID but with a shorter distance.
          </li>
          <li>
            The root ID and distance are equal, but the sending switch has a
            smaller ID
          </li>
        </ul>
        <p>
          If the new message is better than the currently recorded information,
          the switch discards the old information and saves the new information.
          However, it first adds 1 to the distance-to-root field since the
          switch is one hop farther away from the root than the switch that sent
          the message.
        </p>
        <p>
          When a switch receives a configuration message indicating that it is
          not the root—that is, a message from a switch with a smaller ID—the
          switch stops generating configuration messages on its own and instead
          only forwards configuration messages from other switches, after first
          adding 1 to the distance field. Like wise, when a switch receives a
          configuration message that indicates it is not the designated switch
          for that port—that is, a message from a switch that is closer to the
          root or equally far from the root but with a smaller ID—the switch
          stops sending configuration messages over that port. Thus, when the
          system stabilizes, only the root switch is still generating
          configuration messages, and the other switches are forwarding these
          mes sages only over ports for which they are the designated switch. At
          this point, a spanning tree has been built, and all the switches are
          in agreement on which ports are in use for the spanning tree. Only
          those ports may be used for forwarding data packets.
        </p>
        <p>
          Let’s see how this works with an example. Consider what would happen
          in Figure 3.12 if the power had just been restored to a campus, so
          that all the switches boot at about the same time. All the switches
          would start off by claiming to be the root. We denote a configuration
          message from node X in which it claims to be distance d from root node
          Y as (Y,d,X). Focusing on the activity at S3, a sequence of events
          would unfold as follows:
        </p>
        <ol>
          <li>S3 receives (S2, 0, S2).</li>
          <li>Since 2 < 3, S3 accepts S2 as root.</li>
          <li>
            S3 adds one to the distance advertised by S2 (0) and thus sends (S2,
            1, S3) toward S5.
          </li>
          <li>
            Meanwhile, S2 accepts S1 as root because it has the lower ID, and it
            sends (S1, 1, S2) toward S3.
          </li>
          <li>S5 accepts S1 as root and sends (S1, 1, S5) toward S3.</li>
          <li>
            S3 accepts S1 as root, and it notes that both S2 and S5 are closer
            to the root than it is, but S2 has the smaller id, so it remains on
            S3’s path to the root.
          </li>
        </ol>
        <p>
          This leaves S3 with active ports as shown in Figure 3.12. Note that
          Hosts A an B are not able to commu nication over the shortest path
          (via S5) because frames have to “flow up the tree and back down,” but
          that’s the price you pay to avoid loops.
        </p>
        <p>
          Even after the system has stabilized, the root switch continues to
          send configuration messages periodically, and the other switches
          continue to forward these messages as just described. Should a
          particular switch fail, the downstream switches will not receive these
          configuration messages, and after waiting a specified period of time
          they will once again claim to be the root, and the algorithm will kick
          in again to elect a new root and new designated switches.
        </p>
        <p>
          Oneimportant thing to notice is that although the algorithm is able to
          reconfigure the spanning tree whenever a switch fails, it is not able
          to forward frames over alternative paths for the sake of routing
          around a congested switch.
        </p>
      </section>
      <section>
        <h2>3.2.4 Broadcast and Multicast</h2>
        <p>
          The preceding discussion focuses on how switches forward unicast
          frames from one port to another. Since the goal of a switch is to
          transparently extend a LAN across multiple networks, and since most
          LANs support both broadcast and multicast, then switches must also
          support these two features. Broadcast is simple—each switch forwards a
          frame with a destination broadcast address out on each active
          (selected) port other than the one on which the frame was received.
        </p>
        <p>
          Multicast can be implemented in exactly the same way, with each host
          deciding for itself whether or not to accept the message. This is
          exactly what is done in practice. Notice, however, that since not all
          hosts are a member of any particular multicast group, it is possible
          to do better. Specifically, the spanning tree algorithm can be
          extended to prune networks over which multicast frames need not be
          forwarded. Consider a frame sent to group M by a host A in Figure
          3.12. If host C does not belong to group M, then there is no need for
          switch S4 to forward the frames over that network.
        </p>
        <p>
          Howwould agiven switch learn whether it should forward a multicast
          frame over a given port? It learns ex actly the same way that a switch
          learns whether it should forward a unicast frame over a particular
          port—by observing the source addresses that it receives over that
          port. Of course, groups are not typically the source of frames, so we
          have to cheat a little. In particular, each host that is a member of
          group M must periodically send a frame with the address for group M in
          the source field of the frame header. This frame would have as its
          destination address the multicast address for the switches.
        </p>
        <p>
          Although the multicast extension just described was once proposed, it
          was not widely adopted. Instead, multicast is implemented in exactly
          the same way as broadcast.
        </p>
      </section>
      <section>
        <h2>3.2.5 Virtual LANs (VLANs)</h2>
        <p>
          One limitation of switches is that they do not scale. It is not
          realistic to connect more than a few switches, where in practice few
          typically means “tens of.” One reason for this is that the spanning
          tree algorithm scales linearly; that is, there is no provision for
          imposing a hierarchy on the set of switches. A second reason is that
          switches forward all broadcast frames. While it is reasonable for all
          hosts within a limited setting (say, a department) to see each other’s
          broadcast messages, it is unlikely that all the hosts in a larger
          environment (say, a large company or university) would want to have to
          be bothered by each other’s broadcast messages. Said another way,
          broadcast does not scale, and as a consequence L2-based networks do
          not scale.
        </p>
        <p>
          One approach to increasing the scalability is the virtual LAN (VLAN).
          VLANs allow a single extended LAN to be partitioned into several
          seemingly separate LANs. Each virtual LAN is assigned an identifier
          (sometimes called a color), and packets can only travel from one
          segment to another if both segments have the same identifier. This has
          the effect of limiting the number of segments in an extended LAN that
          will receive any given broadcast packet.
        </p>

        <figure>
          <img
            src="images/fig3-13.PNG"
            alt="Two virtual LANs share a common backbone"
            class="styled-image"
          />
          <figcaption>
            Figure 3.13: Two virtual LANs share a common backbone.
          </figcaption>
        </figure>

        <p>
          We can see how VLANs work with an example. Figure 3.13 shows four
          hosts and two switches. In the absence of VLANs, any broadcast packet
          from any host will reach all the other hosts. Now let’s suppose that
          we define the segments connected to hosts W and X as being in one
          VLAN, which we’ll call VLAN 100. We also define the segments that
          connect to hosts Y and Z as being in VLAN 200. To do this, we need to
          configure a VLAN ID on each port of switches S1 and S2. The link
          between S1 and S2 is considered to be in both VLANs.
        </p>
        <p>
          When a packet sent by host X arrives at switch S2, the switch observes
          that it came in a port that was configured as being in VLAN 100. It
          inserts a VLAN header between the Ethernet header and its payload. The
          interesting part of the VLAN header is the VLAN ID; in this case, that
          ID is set to 100. The switch now applies its normal rules for
          forwarding to the packet, with the extra restriction that the packet
          may not be sent out an interface that is not part of VLAN 100. Thus,
          under no circumstances will the packet—even a broadcast packet—be sent
          out the interface to host Z, which is in VLAN 200. The packet,
          however, is forwarded on to switch S1, which follows the same rules
          and thus may forward the packet to host W but not to host Y.
        </p>
        <p>
          An attractive feature of VLANs is that it is possible to change the
          logical topology without moving any wires or changing any addresses.
          For example, if we wanted to make the link that connects to host Z be
          part of VLAN 100 and thus enable X, W, and Z to be on the same virtual
          LAN, then we would just need to change one piece of configuration on
          switch S2.
        </p>
        <p>
          Supporting VLANs requires a fairly simple extension to the original
          802.1 header specification, inserting a 12-bit VLAN ID (VID) field
          between the SrcAddr and Type fields, as shown in Figure 3.14. (This
          VID is typically referred to as a VLAN Tag.) There are actually
          32-bits inserted in the middle of the header, but the first 16-bits
          are used to preserve backwards compatibility with the original
          specification (they use Type = 0x8100toindicatethatthisframeincludes
          the VLANextension); the other four bits hold control information used
          to prioritize frames. This means it is possible to map 212 = 4096
          virtual networks onto a single physical LAN.
        </p>
        <figure>
          <img
            src="images/fig3-14.PNG"
            alt="802.1Q VLAN tag embedded within an Ethernet header"
            class="styled-image"
          />
          <figcaption>
            Figure 3.14: 802.1Q VLAN tag embedded within an Ethernet (802.1)
            header.
          </figcaption>
        </figure>
        <p>
          We conclude this discussion by observing there is another limitation
          of networks built by interconnecting L2 switches: lack of support for
          heterogeneity. That is, switches are limited in the kinds of networks
          they can interconnect. In particular, switches make use of the
          network’s frame header and so can support only networks that have
          exactly the same format for addresses. For example, switches can be
          used to connect Ethernet and 802.11 based networks to another, since
          they share a common header format, but switches do not readily
          generalize to other kinds of networks with different addressing
          formats, such as ATM, SONET, PON, or the cellular network. The next
          section explains how to address this limitation, as well as to scale
          switched networks to even larger sizes.
        </p>
      </section>
      <section>
        <h2>3.3 Internet (IP)</h2>
        In the previous section, we saw that it was possible to build reasonably
        large LANs using bridges and LAN switches, but that such approaches were
        limited in their ability to scale and to handle heterogeneity. In this
        section, we explore some ways to go beyond the limitations of bridged
        networks, enabling us to build large, highly heterogeneous networks with
        reasonably efficient routing. We refer to such networks as
        internetworks. We’ll continue the discussion of how to build a truly
        global internetwork in the next chapter, but for now we’ll explore the
        basics. We start by considering more carefully what the word
        internetwork means.
      </section>
      <section>
        <h2>3.3.1 What Is an Internetwork?</h2>
        <p>
          We use the term <strong><em>internetwork</em></strong
          >, or sometimes just <strong><em>internet</em></strong> with a
          lowercase i, to refer to an arbitrary collection of networks
          interconnected to provide some sort of host-to-host packet delivery
          service. For example, a corporation with many sites might construct a
          private internetwork by interconnecting the LANs at their different
          sites with point-to-point links leased from the phone company. When we
          are talking about the widely used global internetwork to which a large
          percentage of networks are now connected, we call it the Internet with
          a capital I. In keeping with the first-principles approach of this
          book, we mainly want you to learn about the principles of “lowercase
          i” internetworking, but we illustrate these ideas with real-world
          examples from the “big I” Internet.
        </p>
        <p>
          Another piece of terminology that can be confusing is the difference
          between networks, subnetworks, and internetworks. We are going to
          avoid subnetworks (or subnets) altogether until a later section. For
          now, we use network to mean either a directly connected or a switched
          network of the kind described in the previous section and the previous
          chapter. Such a network uses one technology, such as 802.11 or
          Ethernet. An internetwork is an interconnected collection of such
          networks. Sometimes, to avoid ambiguity, we refer to the underlying
          networks that we are interconnecting as physical networks. An internet
          is a logical network built out of a collection of physical networks.
          In this context, a collection of Ethernet segments connected by
          bridges or switches would still be viewed as a single network.
        </p>
        <figure>
          <img
            src="images/fig3-15.PNG"
            alt="Simple internetwork with hosts and routers"
            class="styled-image"
          />
          <figcaption>
            Figure 3.15: A simple internetwork. H denotes a host and R denotes a
            router.
          </figcaption>
        </figure>

        <p>
          Figure 3.15 shows an example internetwork. An internetwork is often
          referred to as a “network of networks” because it is made up of lots
          of smaller networks. In this figure, we see Ethernets, a wireless
          network, and a point-to-point link. Each of these is a
          single-technology network. The nodes that interconnect the networks
          are called routers. They are also sometimes called gateways, but since
          this term has several other connotations, we restrict our usage to
          router.
        </p>
        <p>
          The <strong><em>Internet Protocol</em></strong> is the key tool used
          today to build scalable, heterogeneous internetworks. It was
          originally known as the Kahn-Cerf protocol after its inventors. One
          way to think of IP is that it runs on all the nodes (both hosts and
          routers) in a collection of networks and defines the infrastructure
          that allows these nodes and networks to function as a single logical
          internetwork. For example, Figure 3.16 shows how hosts H5 and H8 are
          logically connected by the internet in Figure 3.15, including the
          protocol graph running on each node. Note that higher-level protocols,
          such as TCP and UDP, typically run on top of IP on the hosts.
        </p>
        <figure>
          <img
            src="images/fig3-16.PNG"
            alt="Protocol layers connecting H5 to H8 over Ethernet"
            class="styled-image"
          />
          <figcaption>
            Figure 3.16: A simple internetwork, showing the protocol layers used
            to connect H5 to H8 in the above figure. ETH is the protocol that
            runs over the Ethernet.
          </figcaption>
        </figure>

        <p>
          The rest of this and the next chapter are about various aspects of IP.
          While it is certainly possible to build an internetwork that does not
          use IP—and in fact, in the early days of the Internet there were
          alternative solutions—IP is the most interesting case to study simply
          because of the size of the Internet. Said another way, it is only the
          IP Internet that has really faced the issue of scale. Thus, it
          provides the best case study of a scalable internetworking protocol.
        </p>

        <section aria-labelledby="l2-l3-networks" class="info-box">
          <h3 id="l2-l3-networks">L2 vs L3 Networks</h3>
          <p>
            As seen in the previous section, an Ethernet can be treated as a
            point-to-point link interconnecting a pair of switches, with a mesh
            of interconnected switches forming a Switched Ethernet. This
            configuration is also known as an L2 Network.
          </p>
          <p>
            But as we’ll discover in this section, an Ethernet (even when
            arranged in a point-to-point configuration rather than a shared
            CSMA/CD network) can be treated as a network interconnecting a pair
            of routers, with a mesh of such routers forming an Internet. This
            configuration is also known as an L3 Network.
          </p>
          <p>
            Confusingly, this is because a point-to-point Ethernet is both a
            link and a network (albeit a trivial two-node network in the second
            case), depending on whether it’s connected to a pair of L2 switches
            running the spanning tree algorithm, or to a pair of L3 routers
            running IP (plus the routing protocols described later in this
            chapter). Why pick one configuration over the other? It partly
            depends on whether you want the network to be a single broadcast
            domain (if yes, pick L2), and whether you want the hosts connected
            to the network to be on different networks (if yes, select L3).
          </p>
          <p>
            The good news is that when you fully understand the implications of
            this duality, you will have cleared a major hurdle in mastering
            modern packet-switched networks.
          </p>
        </section>
      </section>
      <section>
        <h2>3.3.2 Service Model</h2>
        <p>
          A good place to start when you build an internetwork is to define its
          service model, that is, the host-to host services you want to provide.
          The main concern in defining a service model for an internetwork is
          that we can provide a host-to-host service only if this service can
          somehow be provided over each of the underlying physical networks. For
          example, it would be no good deciding that our internetwork service
          model was going to provide guaranteed delivery of every packet in 1 ms
          or less if there were underlying network technologies that could
          arbitrarily delay packets. The philosophy used in defining the IP
          service model, therefore, was to make it undemanding enough that just
          about any network technology that might turn up in an internetwork
          would be able to provide the necessary service. The IP service model
          can be thought of as having two parts: an addressing scheme, which
          provides a way to identify all hosts in the internetwork, and a
          datagram (connectionless) model of data delivery. This service model
          is sometimes called best effort because, although IP makes every
          effort to deliver datagrams, it makes no guarantees. We postpone a
          discussion of the addressing scheme for now and look first at the data
          delivery model.
        </p>
      </section>
      <section>
        <h2>Datagram Delivery</h2>
        <p>
          The IP datagram is fundamental to the Internet Protocol. Recall an
          earlier section that a datagram is a type of packet that happens to be
          sent in a connectionless manner over a network. Every datagram carries
          enough information to let the network forward the packet to its
          correct destination; there is no need for any advance setup mechanism
          to tell the network what to do when the packet arrives. You just send
          it, and the network makes its best effort to get it to the desired
          destination. The “best-effort” part means that if something goes wrong
          and the packet gets lost, corrupted, misdelivered, or in any way fails
          to reach its intended destination, the network does nothing—it made
          its best effort, and that is all it has to do. It does not make any
          attempt to recover from the failure. This is sometimes called an
          unreliable service.
        </p>
        <p>
          Best-effort, connectionless service is about the simplest service you
          could ask for from an internetwork, and this is a great strength. For
          example, if you provide best-effort service over a network that
          provides a reliable service, then that’s fine—you end up with a
          best-effort service that just happens to always deliver the packets.
          If, on the other hand, you had a reliable service model over an
          unreliable network, you would have to put lots of extra functionality
          into the routers to make up for the deficiencies of the underlying
          network. Keeping the routers as simple as possible was one of the
          original design goals of IP.
        </p>
        <p>
          The ability of IP to “run over anything” is frequently cited as one of
          its most important characteristics. It is noteworthy that many of the
          technologies over which IP runs today did not exist when IP was
          invented. So far, no networking technology has been invented that has
          proven too bizarre for IP; in principle, IP can run over a network
          that transports messages using carrier pigeons.
        </p>
        <p>
          Best-effort delivery does not just mean that packets can get lost.
          Sometimes they can get delivered out of order, and sometimes the same
          packet can get delivered more than once. The higher-level protocols or
          applications that run above IP need to be aware of all these possible
          failure modes.
        </p>
      </section>
      <section>
        <h2>Packet Format</h2>
        <p>
          Clearly, a key part of the IP service model is the type of packets
          that can be carried. The IP datagram, like most packets, consists of a
          header followed by a number of bytes of data. The format of the header
          is shown in Figure 3.17. Note that we have adopted a different style
          of representing packets than the one we used in previous chapters.
          This is because packet formats at the internetworking layer and above,
          where we will be focusing our attention for the next few chapters, are
          almost invariably designed to align on 32-bit boundaries to simplify
          the task of processing them in software. Thus, the common way of
          representing them (used in Internet Requests for Comments, for
          example) is to draw them as a succession of 32-bit words. The top word
          is the one transmitted first, and the leftmost byte of each word is
          the one transmitted first. In this representation, you can easily
          recognize fields that are a multiple of 8 bits long. On the odd
          occasion when fields are not an even multiple of 8 bits, you can
          determine the field lengths by looking at the bit positions marked at
          the top of the packet.
        </p>
        <figure>
          <img
            src="images/fig3-17.PNG"
            alt="IPv4 packet header"
            class="styled-image"
          />
          <figcaption>Figure 3.17: IPv4 packet header.</figcaption>
        </figure>
        <p>
          Looking at each field in the IP header, we see that the “simple” model
          of best-effort datagram delivery still has some subtle features. The
          Version field specifies the version of IP. The still-assumed version
          of IP is 4, which is typically called IPv4. Observe that putting this
          field right at the start of the datagram makes it easy for everything
          else in the packet format to be redefined in subsequent versions; the
          header processing software starts off by looking at the version and
          then branches off to process the rest of the packet according to the
          appropriate format. The next field, HLen, specifies the length of the
          header in 32-bit words. When there are no options, which is most of
          the time, the header is 5 words (20 bytes) long. The 8-bit TOS (type
          of service) field has had a number of different definitions over the
          years, but its basic function is to allow packets to be treated
          differently based on application needs. For example, the TOS value
          might determine whether or not a packet should be placed in a special
          queue that receives low delay.
        </p>
        <p>
          The next 16 bits of the header contain the Length of the datagram,
          including the header. Unlike the HLen field, the Length field counts
          bytes rather than words. Thus, the maximum size of an IP datagram is
          65,535 bytes. The physical network over which IP is running, however,
          may not support such long packets. For this reason, IP supports a
          fragmentation and reassembly process. The second word of the header
          contains information about fragmentation, and the details of its use
          are presented in the following section entitled “Fragmentation and
          Reassembly.”
        </p>
        <p>
          Moving on to the third word of the header, the next byte is the
          <em><strong>TTL</strong></em> (time to live) field. Its name reflects
          its historical meaning rather than the way it is commonly used today.
          The intent of the field is to catch packets that have been going
          around in routing loops and discard them, rather than let them consume
          resources indefinitely. Originally, <em><strong>TTL</strong></em> was
          set to a specific number of seconds that the packet would be allowed
          to live, and routers along the path would decrement this field until
          it reached 0. However, since it was rare for a packet to sit for as
          long as 1 second in a router, and routers did not all have access to a
          common clock, most routers just decremented the
          <em><strong>TTL</strong></em> by 1 as they forwarded the packet. Thus,
          it became more of a hop count than a timer, which is still a perfectly
          good way to catch packets that are stuck in routing loops. One
          subtlety is in the initial setting of this field by the sending host:
          Set it too high and packets could circulate rather a lot before
          getting dropped; set it too low and they may not reach their
          destination. The value 64 is the current default.
        </p>
        <p>
          The <em><strong>Protocol</strong></em> field is simply a
          demultiplexing key that identifies the higher-level protocol to which
          this IP packet should be passed. There are values defined for the TCP
          (Transmission Control Protocol—6), UDP (User Datagram Protocol—17),
          and many other protocols that may sit above IP in the protocol graph.
        </p>
        <p>
          The <em><strong>Checksum</strong></em> is calculated by considering
          the entire IP header as a sequence of 16-bit words, adding them up
          using ones’ complement arithmetic, and taking the ones’ complement of
          the result. Thus, if any bit in the header is corrupted in transit,
          the checksum will not contain the correct value upon receipt of the
          packet. Since a corrupted header may contain an error in the
          destination address—and, as a result, may have been misdelivered—it
          makes sense to discard any packet that fails the checksum. It should
          be noted that this type of checksum does not have the same strong
          error detection properties as a CRC, but it is much easier to
          calculate in software.
        </p>
        <p>
          The last two required fields in the header are the SourceAddr and the
          DestinationAddr for the packet. The latter is the key to datagram
          delivery: Every packet contains a full address for its intended
          destination so that forwarding decisions can be made at each router.
          The source address is required to allow recipients to decide if they
          want to accept the packet and to enable them to reply. IP addresses
          are discussed in a later section—for now, the important thing to know
          is that IP defines its own global address space, independent of
          whatever physical networks it runs over. As we will see, this is one
          of the keys to supporting heterogeneity.
        </p>
        <p>
          Finally, there may be a number of options at the end of the header.
          The presence or absence of options may be determined by examining the
          header length (HLen) field. While options are used fairly rarely, a
          complete IP implementation must handle them all.
        </p>
      </section>
      <section>
        <h2>Fragmentation and Reassembly</h2>
        <p>
          One of the problems of providing a uniform host-to-host service model
          over a heterogeneous collection of networks is that each network
          technology tends to have its own idea of how large a packet can be.
          For example, classic Ethernet can accept packets up to 1500 bytes
          long, but modern-day variants can deliver larger (jumbo) packets that
          carry up to 9000 bytes of payload. This leaves two choices for the IP
          service model: Make sure that all IP datagrams are small enough to fit
          inside one packet on any network technology, or provide a means by
          which packets can be fragmented and reassembled when they are too big
          to go over a given network technology. The latter turns out to be a
          good choice, especially when you consider the fact that new network
          technologies are always turning up, and IP needs to run over all of
          them; this would make it hard to pick a suitably small bound on
          datagram size. This also means that a host will not send needlessly
          small packets, which wastes bandwidth and consumes processing
          resources by requiring more headers per byte of data sent.
        </p>
        <p>
          The central idea here is that every network type has a maximum
          transmission unit (MTU), which is the largest IP datagram that it can
          carry in a frame.1 Note that this value is smaller than the largest
          packet size on that network because the IP datagram needs to fit in
          the payload of the link-layer frame.
        </p>
        <p>
          When a host sends an IP datagram, therefore, it can choose any size
          that it wants. A reasonable choice is the MTU of the network to which
          the host is directly attached. Then, fragmentation will only be
          necessary if the path to the destination includes a network with a
          smaller MTU. Should the transport protocol that sits on top of IP give
          IP a packet larger than the local MTU, however, then the source host
          must fragment it.
        </p>
        <p>
          Fragmentation typically occurs in a router when it receives a datagram
          that it wants to forward over a network that has an MTU that is
          smaller than the received datagram. To enable these fragments to be
          reassembled at the receiving host, they all carry the same identifier
          in the Ident field. This identifier is chosen by the sending host and
          is intended to be unique among all the datagrams that might arrive at
          the destination from this source over some reasonable time period.
          Since all fragments of the original datagram contain this identifier,
          the reassembling host will be able to recognize those fragments that
          go together. Should all the fragments not arrive at the receiving
          host, the host gives up on the reassembly process and discards the
          fragments that did arrive. IP does not attempt to recover from missing
          fragments.
        </p>
        <figure>
          <img
            src="images/fig3-18.PNG"
            alt="IP datagrams traversing physical networks"
            class="styled-image"
          />
          <figcaption>
            Figure 3.18: IP datagrams traversing the sequence of physical
            networks graphed in the earlier figure.
          </figcaption>
        </figure>
        <p>
          To see what this all means, consider what happens when host H5 sends a
          datagram to host H8 in the example internet shown in Figure 3.15.
          Assuming that the MTU is 1500 bytes for the two Ethernets and the
          802.11 network, and 532 bytes for the point-to-point network, then a
          1420-byte datagram (20-byte IP header plus 1400 bytes of data) sent
          from H5 makes it across the 802.11 network and the first Ethernet
          without fragmen tation but must be fragmented into three datagrams at
          router R2. These three fragments are then forwarded by router R3
          across the second Ethernet to the destination host. This situation is
          illustrated in Figure 3.18.
        </p>
        <p>This figure also serves to reinforce two important points:</p>
        <ol>
          <li>
            Each fragment is itself a self-contained IP datagram that is
            transmitted over a sequence of physical networks, independent of the
            other fragments.
          </li>
          <li>
            Each IP datagram is re-encapsulated for each physical network over
            which it travels.
          </li>
        </ol>

        <p>
          The fragmentation process can be understood in detail by looking at
          the header fields of each datagram, as 1 In ATMnetworks, the MTU is,
          fortunately, much larger than a single cell, as ATM has its own
          fragmentation and reassembly mechanism. The link-layer frame in ATM is
          called a convergence-sublayer protocol data unit (CS-PDU).
        </p>

        <figure>
          <img
            src="images/fig3-19.PNG"
            alt="Header fields used in IP fragmentation"
            class="styled-image"
          />
          <figcaption>
            Figure 3.19: Header fields used in IP fragmentation: (a)
            unfragmented packet; (b) fragmented packets.
          </figcaption>
        </figure>
        <p>
          is done in Figure 3.19. The unfragmented packet, shown at the top, has
          1400 bytes of data and a 20-byte IP header. When the packet arrives at
          router R2, which has an MTU of 532 bytes, it has to be fragmented. A
          532-byte MTU leaves 512 bytes for data after the 20-byte IP header, so
          the first fragment contains 512 bytes of data. The router sets the M
          bit in the Flagsfield(seeFigure3.17), meaningthat there are more
          fragments to follow, and it sets the Offset to 0, since this fragment
          contains the first part of the original datagram. The data carried in
          the second fragment starts with the 513th byte of the original data,
          so the Offset field in this header is set to 64, which is 512/8. Why
          the division by 8? Because the designers of IP decided that
          fragmentation should always happen on 8-byte boundaries, which means
          that the Offset field counts 8-byte chunks, not bytes. (We leave it as
          an exercise for you to figure out why this design decision was made.)
          The third fragment contains the last 376 bytes of data, and the offset
          is now 2 × 512/8 = 128. Since this is the last fragment, the M bit is
          not set.
        </p>
        <p>
          Observe that the fragmentation process is done in such a way that it
          could be repeated if a fragment arrived at another network with an
          even smaller MTU. Fragmentation produces smaller, valid IP datagrams
          that can be readily reassembled into the original datagram upon
          receipt, independent of the order of their arrival. Reassembly is done
          at the receiving host and not at each router.
        </p>
        <p>
          IP reassembly is far from a simple process. For example, if a single
          fragment is lost, the receiver will still attempt to reassemble the
          datagram, and it will eventually give up and have to garbage-collect
          the resources that were used to perform the failed reassembly. Getting
          a host to tie up resources needlessly can be the basis of a
          denial-of-service attack.
        </p>
        <p>
          For this reason, among others, IP fragmentation is generally
          considered a good thing to avoid. Hosts are now strongly encouraged to
          perform “path MTU discovery,” a process by which fragmentation is
          avoided by sending packets that are small enough to traverse the link
          with the smallest MTU in the path from sender to receiver.
        </p>
      </section>
      <section>
        <h2>3.3.3 Global Addresses</h2>
        <p>
          In the above discussion of the IP service model, we mentioned that one
          of the things that it provides is an addressing scheme. After all, if
          you want to be able to send data to any host on any network, there
          needs to be a way of identifying all the hosts. Thus, we need a global
          addressing scheme—one in which no two hosts have the same address.
          Global uniqueness is the first property that should be provided in an
          addressing scheme.
        </p>
        <p>
          Ethernet addresses are globally unique, but that alone does not
          suffice for an addressing scheme in a large internetwork. Ethernet
          addresses are also flat, which means that they have no structure and
          provide very few clues to routing protocols. (In fact, Ethernet
          addresses do have a structure for the purposes of as signment—the
          first 24 bits identify the manufacturer—but this provides no useful
          information to routing protocols since this structure has nothing to
          do with network topology.) In contrast, IP addresses are hierar
          chical, by which we mean that they are made up of several parts that
          correspond to some sort of hierarchy in the internetwork.
          Specifically, IP addresses consist of two parts, usually referred to
          as a network part and a host part. This is a fairly logical structure
          for an internetwork, which is made up of many interconnected networks.
          The network part of an IP address identifies the network to which the
          host is attached; all hosts attached to the same network have the same
          network part in their IP address. The host part then identifies each
          host uniquely on that particular network. Thus, in the simple
          internetwork of Figure 3.15, the addresses of the hosts on network 1,
          for example, would all have the same network part and different host
          parts.
        </p>
        <p>
          Note that the routers in Figure 3.15 are attached to two networks.
          They need to have an address on each network, one for each interface.
          For example, router R1, which sits between the wireless network and an
          Ethernet, has an IP address on the interface to the wireless network
          whose network part is the same as all the hosts on that network. It
          also has an IP address on the interface to the Ethernet that has the
          same network part as the hosts on that Ethernet. Thus, bearing in mind
          that a router might be implemented as a host with two network
          interfaces, it is more precise to think of IP addresses as belonging
          to interfaces than to hosts.
        </p>
        <p>
          Now, what do these hierarchical addresses look like? Unlike some other
          forms of hierarchical address, the sizes of the two parts are not the
          same for all addresses. Originally, IP addresses were divided into
          three different classes, as shown in Figure 3.20, each of which
          defines different-sized network and host parts. (There are also class
          D addresses that specify a multicast group and class E addresses that
          are currently unused.) In all cases, the address is 32 bits long.
        </p>
        <p>
          The class of an IP address is identified in the most significant few
          bits. If the first bit is 0, it is a class A address. If the first bit
          is 1 and the second is 0, it is a class B address. If the first two
          bits are 1 and the third is 0, it is a class C address. Thus, of the
          approximately 4 billion possible IP addresses, half are class A,
          one-quarter are class B, and one-eighth are class C. Each class
          allocates a certain number of bits for the network part of the address
          and the rest for the host part. Class A networks have 7 bits for the
          network part and 24 bits for the host part, meaning that there can be
          only 126 class A networks (the values 0 and 127 are reserved), but
          each of them can accommodate up to 224 − 2 (about 16 million) hosts
          (again, there are two reserved values). Class B addresses allocate 14
          bits for the network and 16 bits for the host, meaning that each class
          B network has room for 65,534 hosts. Finally, class C addresses have
          only 8 bits for the host and 21 for the network part. Therefore, a
          class C network can have only 256 unique host identifiers, which means
          only 254 attached hosts (one host identifier, 255, is reserved for
          broadcast, and 0 is not a valid host number). However, the addressing
          scheme supports 221 class C networks.
        </p>
        <figure>
          <img
            src="images/fig3-20.PNG"
            alt="IP address classes A, B, and C"
            class="styled-image"
          />
          <figcaption>
            Figure 3.20: IP addresses: (a) class A; (b) class B; (c) class C.
          </figcaption>
        </figure>
        <p>
          On the face of it, this addressing scheme has a lot of flexibility,
          allowing networks of vastly different sizes to be accommodated fairly
          efficiently. The original idea was that the Internet would consist of
          a small number of wide area networks (these would be class A
          networks), a modest number of site- (campus ) sized networks (these
          would be class B networks), and a large number of LANs (these would be
          class C networks). However, it turned out not to be flexible enough,
          as we will see in a moment. Today, IP addresses are normally
          “classless”; the details of this are explained below.
        </p>
        <p>
          Before we look at how IP addresses get used, it is helpful to look at
          some practical matters, such as how you write them down. By
          convention, IP addresses are written as four decimal integers
          separated by dots. Each integer represents the decimal value contained
          in 1 byte of the address, starting at the most significant. For
          example, the address of the computer on which this sentence was typed
          is <strong>171.69.210.245</strong>. It is important not to confuse IP
          addresses with Internet domain names, which are also hierarchical.
          Domain names tend to be ASCII strings separated by dots, such as
          cs.princeton.edu. The important thing about IP addresses is that they
          are what is carried in the headers of IP packets, and it is those
          addresses that are used in IP routers to make forwarding decisions.
        </p>
      </section>
      <section>
        <h2></h2>
        <h2>3.3.4 Datagram Forwarding in IP</h2>
        <p>
          We are now ready to look at the basic mechanism by which IP routers
          forward datagrams in an internetwork. Recall from an earlier section
          that forwarding is the process of taking a packet from an input and
          sending it out on the appropriate output, while routing is the process
          of building up the tables that allow the correct output for a packet
          to be determined. The discussion here focuses on forwarding; we take
          up routing in a later section.
        </p>
        <p>
          The main points to bear in mind as we discuss the forwarding of IP
          datagrams are the following:
        </p>
        <ul>
          <li>
            Every IP datagram contains the IP address of the destination host.
          </li>
          <li>
            The network part of an IP address uniquely identifies a single
            physical network that is part of the larger Internet.
          </li>
          <li>
            All hosts and routers that share the same network part of their
            address are connected to the same physical network and can thus
            communicate with each other by sending frames over that network.
          </li>
          <li>
            Every physical network that is part of the Internet has at least one
            router that, by definition, is also connected to at least one other
            physical network; this router can exchange packets with hosts or
            routers on either network.
          </li>
        </ul>
        <p>
          Forwarding IP datagrams can therefore be handled in the following way.
          A datagram is sent from a source host to a destination host, possibly
          passing through several routers along the way. Any node, whether it is
          a host or a router, first tries to establish whether it is connected
          to the same physical network as the destination. To do this, it
          compares the network part of the destination address with the network
          part of the address of each of its network interfaces. (Hosts normally
          have only one interface, while routers normally have two or more,
          since they are typically connected to two or more networks.) If a
          match occurs, then that means that the destination lies on the same
          physical network as the interface, and the packet can be directly
          delivered over that network. A later section explains some of the
          details of this process.
        </p>
        <p>
          If the node is not connected to the same physical network as the
          destination node, then it needs to send the datagram to a router. In
          general, each node will have a choice of several routers, and so it
          needs to pick the best one, or at least one that has a reasonable
          chance of getting the datagram closer to its destination. The router
          that it chooses is known as the next hop router. The router finds the
          correct next hop by consulting its forwarding table. The forwarding
          table is conceptually just a list of (NetworkNum, NextHop)pairs. (As
          wewill see below, forwarding tables in practice often contain some
          additional information related to the next hop.) Normally, there is
          also a default router that is used if none of the entries in the table
          matches the destination’s network number. For a host, it may be quite
          acceptable to have a default router and nothing else—this means that
          all datagrams destined for hosts not on the physical network to which
          the sending host is attached will be sent out through the default
          router.
        </p>
        <p>
          We can describe the datagram forwarding algorithm in the following
          way:
        </p>
        <figure>
          <img
            src="images/code-img3.PNG"
            alt="IP forwarding logic based on destination network number"
            class="styled-image"
          />
          <figcaption>Example of datagram forwarding algorithm.</figcaption>
        </figure>
        <p>
          For a host with only one interface and only a default router in its
          forwarding table, this simplifies to:
        </p>
        <figure>
          <img
            src="images/code-img4.PNG"
            alt="IP forwarding logic based on destination network number"
            class="styled-image"
          />
          <figcaption>Example of datagram forwarding algorithm.</figcaption>
        </figure>
        <p>
          Let’s see how this works in the example internetwork of Figure 3.15.
          First, suppose that H1 wants to send a datagram to H2. Since they are
          on the same physical network, H1 and H2 have the same network number
          in their IP address. Thus, H1 deduces that it can deliver the datagram
          directly to H2 over the Ethernet. The one issue that needs to be
          resolved is how H1 finds out the correct Ethernet address for H2—the
          resolution mechanism described in a later section addresses this
          issue.
        </p>
        <p>
          Now suppose H5 wants to send a datagram to H8. Since these hosts are
          on different physical networks, they have different network numbers,
          so H5 deduces that it needs to send the datagram to a router. R1 is
          the only choice—the default router—so H1 sends the datagram over the
          wireless network to R1. Similarly, R1 knows that it cannot deliver a
          datagram directly to H8 because neither of R1’s interfaces are on the
          same network as H8. Suppose R1’s default router is R2; R1 then sends
          the datagram to R2 over the Ethernet. Assuming R2 has the forwarding
          table shown in Table 3.6, it looks up H8’s network number (network 4)
          and forwards the datagram over the point-to-point network to R3.
          Finally, R3, since it is on the same network as H8, forwards the
          datagram directly to H8.
        </p>
        <table aria-labelledby="table-3-6">
          <caption id="table-3-6">
            <strong>Table 3.6:</strong>
            Forwarding table for Router R2.
          </caption>
          <thead>
            <tr>
              <th>NetworkNum</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>R1</td>
            </tr>
            <tr>
              <td>4</td>
              <td>R3</td>
            </tr>
          </tbody>
        </table>
        <p>
          Note that it is possible to include the information about directly
          connected networks in the forwarding table. For example, we could
          label the network interfaces of router R2 as interface 0 for the
          point-to-point link (network 3) and interface 1 for the Ethernet
          (network 2). Then R2 would have the forwarding table shown in Table
          3.7.
        </p>
        <table aria-labelledby="table-3-7">
          <caption id="table-3-7">
            <strong>Table 3.7:</strong>
            Complete Forwarding table for Router R2.
          </caption>
          <thead>
            <tr>
              <th>NetworkNum</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>R1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Interface 1</td>
            </tr>
            <tr>
              <td>3</td>
              <td>Interface 0</td>
            </tr>
            <tr>
              <td>4</td>
              <td>R3</td>
            </tr>
          </tbody>
        </table>
        <p>
          Thus, for any network number that R2 encounters in a packet, it knows
          pwhat to do. Either that network is directly connected to R2, in which
          case the packet can be delivered to its destination over that network,
          or the network is reachable via some next hop router that R2 can reach
          over a network to which it is connected. In either case, R2 will use
          ARP, described below, to find the MAC address of the node to which the
          packet is to be sent next.
        </p>
        <p>
          The forwarding table used by R2 is simple enough that it could be
          manually configured. Usually, however, these tables are more complex
          and would be built up by running a routing protocol such as one of
          those described in a later section. Also note that, in practice, the
          network numbers are usually longer (e.g., 128.96).
        </p>
        <p>
          We can now see how hierarchical addressing—splitting the address into
          network and host parts—has im proved the scalability of a large
          network. Routers now contain forwarding tables that list only a set of
          network numbers rather than all the nodes in the network. In our
          simple example, that meant that R2 could store the information needed
          to reach all the hosts in the network (of which there were eight) in a
          four-entry table. Even if there were 100 hosts on each physical
          network, R2 would still only need those same four entries. This is a
          good first step (although by no means the last) in achieving
          scalability.
        </p>
        <hr />
        <p><strong> Key Takeaway</strong></p>
        <p>
          This illustrates one of the most important principles of building
          scalable networks: To achieve scalability, you need to reduce the
          amount of information that is stored in each node and that is
          exchanged between nodes. The most common way to do that is
          hierarchical aggregation. IP introduces a two-level hierarchy, with
          networks at the top level and nodes at the bottom level. We have
          aggregated information by letting routers deal only with reaching the
          right network; the information that a router needs to deliver a
          datagram to any node on a given network is represented by a single
          aggregated piece of information. [Next]
        </p>
        <hr />
      </section>
      <section>
        <h2>3.3.5 Subnetting and Classless Addressing</h2>
        <p>
          The original intent of IP addresses was that the network part would
          uniquely identify exactly one physical network. It turns out that this
          approach has a couple of drawbacks. Imagine a large campus that has
          lots of internal networks and decides to connect to the Internet. For
          every network, no matter how small, the site needs at least a class C
          network address. Even worse, for any network with more than 255 hosts,
          they need a class B address. This may not seem like a big deal, and
          indeed it wasn’t when the Internet was first envisioned, but there are
          only a finite number of network numbers, and there are far fewer class
          B addresses than class Cs. Class B addresses tend to be in
          particularly high demand because you never know if your network might
          expand beyond 255 nodes, so it is easier to use a class B address from
          the start than to have to renumber every host when you run out of room
          on a class C network. The problem we observe here is address
          assignment inefficiency: A network with two nodes uses an entire class
          C network address, thereby wasting 253 perfectly useful addresses; a
          class B network with slightly more than 255 hosts wastes over 64,000
          addresses.
        </p>
        <p>
          Assigning one network number per physical network, therefore, uses up
          the IP address space potentially much faster than we would like. While
          we would need to connect over 4 billion hosts to use up all the valid
          addresses, we only need to connect 214 (about 16,000) class B networks
          before that part of the address space runs out. Therefore, we would
          like to find some way to use the network numbers more efficiently.
        </p>
        <p>
          Assigning many network numbers has another drawback that becomes
          apparent when you think about rout ing. Recall that the amount of
          state that is stored in a node participating in a routing protocol is
          proportional to the number of other nodes, and that routing in an
          internet consists of building up forwarding tables that tell a router
          how to reach different networks. Thus, the more network numbers there
          are in use, the big ger the forwarding tables get. Big forwarding
          tables add costs to routers, and they are potentially slower to search
          than smaller tables for a given technology, so they degrade router
          performance. This provides another motivation for assigning network
          numbers carefully.
        </p>
        <p>
          <em><strong>Subnetting</strong></em> provides a first step to reducing
          total number of network numbers that are assigned. The idea is to take
          a single IP network number and allocate the IP addresses with that
          network number to several physical networks, which are now referred to
          as <em><strong>subnets</strong></em
          >. Several things need to be done to make this work. First, the
          subnets should be close to each other. This is because from a distant
          point in the Internet, they will all look like a single network,
          having only one network number between them. This means that a router
          will only be able to select one route to reach any of the subnets, so
          they had better all be in the same general direction. A perfect
          situation in which to use subnetting is a large campus or corporation
          that has many physical networks. From outside the campus, all you need
          to know to reach any subnet inside the campus is where the campus
          connects to the rest of the Internet. This is often at a single point,
          so one entry in your forwarding table will suffice. Even if there are
          multiple points at which the campus is connected to the rest of the
          Internet, knowing how to get to one point in the campus network is
          still a good start.
        </p>
        <p>
          The mechanism by which a single network number can be shared among
          multiple networks involves con figuring all the nodes on each subnet
          with a subnet mask. With simple IP addresses, all hosts on the same
          network must have the same network number. The subnet mask enables us
          to introduce a subnet number; all hosts on the same physical network
          will have the same subnet number, which means that hosts may be on
          different physical networks but share a single network number. This
          concept is illustrated in Figure 3.21.
        </p>
        <figure>
          <img
            src="images/fig3-21.PNG"
            alt="Subnet addressing diagram"
            class="styled-image"
          />
          <figcaption>Figure 3.21: Subnet addressing.</figcaption>
        </figure>
        <p>
          What subnetting means to a host is that it is now configured with both
          an IP address and a subnet mask for the subnet to which it is
          attached. For example, host H1 in Figure 3.22 is configured with an
          address of 128.96.34.15 and a subnet mask of 255.255.255.128. (All
          hosts on a given subnet are configured with the same mask; that is,
          there is exactly one subnet mask per subnet.) The bitwise AND of these
          two numbers defines the subnet number of the host and of all other
          hosts on the same subnet. In this case, 128.96.34.15
          AND255.255.255.128 equals 128.96.34.0, so this is the subnet number
          for the topmost subnet in the figure.
        </p>
        <p>
          When the host wants to send a packet to a certain IP address, the
          first thing it does is to perform a bitwise ANDbetween its own subnet
          mask and the destination IP address. If the result equals the subnet
          number of the sending host, then it knows that the destination host is
          on the same subnet and the packet can be delivered
        </p>
        <figure>
          <img
            src="images/fig3-22.PNG"
            alt="An example of subnetting"
            class="styled-image"
          />
          <figcaption>Figure 3.22: An example of subnetting.</figcaption>
        </figure>
        <p>
          directly over the subnet. If the results are not equal, the packet
          needs to be sent to a router to be forwarded to another subnet. For
          example, if H1 is sending to H2, then H1 ANDs its subnet mask
          (255.255.255.128) with the address for H2 (128.96.34.139) to obtain
          128.96.34.128. This does not match the subnet number for H1
          (128.96.34.0) so H1 knows that H2 is on a different subnet. Since H1
          cannot deliver the packet to H2 directly over the subnet, it sends the
          packet to its default router R1.
        </p>
        <p>
          The forwarding table of a router also changes slightly when we
          introduce subnetting. Recall that we pre viously had a forwarding
          table that consisted of entries of the form (NetworkNum, NextHop). To
          support subnetting, the table must now hold entries of the form
          (SubnetNumber, SubnetMask, NextHop). To find the right entry in the
          table, the router ANDs the packet’s destination address with the
          SubnetMaskfor each entry in turn; if the result matches the
          SubnetNumber of the entry, then this is the right entry to use, and it
          forwards the packet to the next hop router indicated. In the example
          network of Figure 3.22, router R1 would have the entries shown in
          Table 3.8.
        </p>
        <table aria-labelledby="table-3-8">
          <caption id="table-3-8">
            <strong>Table 3.8:</strong>
            Example Forwarding Table with Subnetting.
          </caption>
          <thead>
            <tr>
              <th>SubnetNumber</th>
              <th>SubnetMask</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>128.96.34.0</td>
              <td>255.255.255.128</td>
              <td>Interface 0</td>
            </tr>
            <tr>
              <td>128.96.34.128</td>
              <td>255.255.255.128</td>
              <td>Interface 1</td>
            </tr>
            <tr>
              <td>128.96.33.0</td>
              <td>255.255.255.0</td>
              <td>R2</td>
            </tr>
          </tbody>
        </table>
        <p>
          Continuing with the example of a datagram from H1 being sent to H2, R1
          would AND H2’s ad dress (128.96.34.139) with the subnet mask of the
          first entry (255.255.255.128) and compare the result (128.96.34.128)
          with the network number for that entry (128.96.34.0). Since this is
          not a match, it proceeds to the next entry. This time a match does
          occur, so R1 delivers the datagram to H2 using interface 1, which is
          the interface connected to the same network as H2.
        </p>
        <p>
          Wecan now describe the datagram forwarding algorithm in the following
          way:
        </p>
        <figure>
          <img
            src="images/code-img5.PNG"
            alt="Forwarding algorithm using subnet mask comparison"
            class="styled-image"
          />
          <figcaption>
            Figure: Datagram forwarding algorithm using subnet-based matching.
            The destination IP address is masked and compared against each
            forwarding table entry to determine whether to deliver the datagram
            directly or forward it to the next-hop router.
          </figcaption>
        </figure>
        <p>
          Although not shown in this example, a default route would usually be
          included in the table and would be used if no explicit matches were
          found. Note that a naive implementation of this algorithm—one
          involving repeated ANDing of the destination address with a subnet
          mask that may not be different every time, and a linear table
          search—would be very inefficient.
        </p>
        <p>
          Animportant consequence of subnetting is that different parts of the
          internet see the world differently. From outside our hypothetical
          campus, routers see a single network. In the example above, routers
          outside the campus see the collection of networks in Figure 3.22 as
          just the network 128.96, and they keep one entry in their forwarding
          tables to tell them how to reach it. Routers within the campus,
          however, need to be able to route packets to the right subnet. Thus,
          not all parts of the internet see exactly the same routing
          information. This is an example of an aggregation of routing
          information, which is fundamental to scaling of the routing system.
          The next section shows how aggregation can be taken to another level.
        </p>
      </section>
      <section>
        <h2>Classless Addressing</h2>
        <p>
          Subnetting has a counterpart, sometimes called supernetting, but more
          often called Classless Interdomain Routing or CIDR, pronounced
          “cider.” CIDR takes the subnetting idea to its logical conclusion by
          essentially doing away with address classes altogether. Why isn’t
          subnetting alone sufficient? In essence, subnetting only allows us to
          split a classful address among multiple subnets, while CIDR allows us
          to coalesce several classful addresses into a single “supernet.” This
          further tackles the address space inefficiency noted above, and does
          so in a way that keeps the routing system from being overloaded.
        </p>
        <p>
          To see how the issues of address space efficiency and scalability of
          the routing system are coupled, consider the hypothetical case of a
          company whose network has 256 hosts on it. That is slightly too many
          for a Class C address, so you would be tempted to assign a class B.
          However, using up a chunk of address space that could address 65535 to
          address 256 hosts has an efficiency of only 256/65,535 = 0.39%. Even
          though subnetting can help us to assign addresses carefully, it does
          not get around the fact that any organization with more than 255
          hosts, or an expectation of eventually having that many, wants a class
          B address.
        </p>
        <p>
          The first way you might deal with this issue would be to refuse to
          give a class B address to any organization that requests one unless
          they can show a need for something close to 64K addresses, and instead
          giving them an appropriate number of class C addresses to cover the
          expected number of hosts. Since we would now be handing out address
          space in chunks of 256 addresses at a time, we could more accurately
          match the amount of address space consumed to the size of the
          organization. For any organization with at least 256 hosts, we can
          guarantee an address utilization of at least 50%, and typically much
          more. (Sadly, even if you can justify a request of a class B network
          number, don’t bother, because they were all spoken for long ago.)
        </p>
        <p>
          This solution, however, raises a problem that is at least as serious:
          excessive storage requirements at the routers. If a single site has,
          say, 16 class C network numbers assigned to it, that means every
          Internet backbone router needs 16 entries in its routing tables to
          direct packets to that site. This is true even if the path to every
          one of those networks is the same. If we had assigned a class B
          address to the site, the same routing information could be stored in
          one table entry. However, our address assignment efficiency would then
          be only 6 x 255 / 65,536 = 6.2%.
        </p>
        <p>
          CIDR, therefore, tries to balance the desire to minimize the number of
          routes that a router needs to know against the need to hand out
          addresses efficiently. To do this, CIDR helps us to aggregate routes.
          That is, it lets us use a single entry in a forwarding table to tell
          us how to reach a lot of different networks. As noted above it does
          this by breaking the rigid boundaries between address classes. To
          understand how this works, consider our hypothetical organization with
          16 class C network numbers. Instead of handing out 16 addresses at
          random, we can hand out a block of contiguous class C addresses.
          Suppose we assign the class C network numbers from 192.4.16 through
          192.4.31. Observe that the top 20 bits of all the addresses in this
          range are the same (11000000 00000100 0001). Thus, what we have
          effectively created is a 20-bit network number—something that is
          between a class B network number and a class C number in terms of the
          number of hosts that it can support. In other words, we get both the
          high address efficiency of handing out addresses in chunks smaller
          than a class B network, and a single network prefix that can be used
          in forwarding tables. Observe that, for this scheme to work, we need
          to hand out blocks of class C addresses that share a common prefix,
          which means that each block must contain a number of class C networks
          that is a power of two.
        </p>
        <p>
          CIDR requires a new type of notation to represent network numbers, or
          prefixes as they are known, because the prefixes can be of any length.
          The convention is to place a /X after the prefix, where X is the
          prefix length in bits. So, for the example above, the 20-bit prefix
          for all the networks 192.4.16 through 192.4.31 is represented as
          192.4.16/20. By contrast, if we wanted to represent a single class C
          network number, which is 24 bits long, we would write it 192.4.16/24.
          Today, with CIDR being the norm, it is more common to hear people talk
          about “slash 24” prefixes than class C networks. Note that
          representing a network address in this way is similar to the(mask,
          value) approach used in subnetting, as long as masks consist of
          contiguous bits starting from the most significant bit (which in
          practice is almost always the case).
        </p>
        <p>
          The ability to aggregate routes at the edge of the network as we have
          just seen is only the first step. Imagine an Internet service provider
          network, whose primary job is to provide Internet connectivity to a
          large number of corporations and campuses (customers). If we assign
          prefixes to the customers in such a way that many different customer
          networks connected to the provider network share a common, shorter
          address prefix, then we can get even greater aggregation of routes.
          Consider the example in Figure 3.23. Assume that eight customers
          served by the provider network have each been assigned adjacent 24-bit
          network prefixes. Those prefixes all start with the same 21 bits.
          Since all of the customers are reachable through the same provider
          network, it can advertise a single route to all of them by just
          advertising the common 21-bit prefix they share. And it can do this
          even if not all the 24-bit prefixes have been handed out, as long as
          the provider ultimately will have the right to hand out those prefixes
          to a customer. One way to accomplish that is to assign a portion of
          address space to the provider in advance and then to let the network
          provider assign addresses from that space to its customers as needed.
          Note that, in contrast to this simple example, there is no need for
          all customer prefixes to be the same length.
        </p>
        <figure>
          <img
            src="images/fig3-23.PNG"
            alt="Route aggregation with CIDR"
            class="styled-image"
          />
          <figcaption>Figure 3.23: Route aggregation with CIDR.</figcaption>
        </figure>
      </section>
      <section>
        <h2>IP Forwarding Revisited</h2>
        <p>
          In all our discussion of IP forwarding so far, we have assumed that we
          could find the network number in a packet and then look up that number
          in a forwarding table. However, now that we have introduced CIDR, we
          need to reexamine this assumption. CIDR means that prefixes may be of
          any length, from 2 to 32 bits. Furthermore, it is sometimes possible
          to have prefixes in the forwarding table that “overlap,” in the sense
          that some addresses may match more than one prefix. For example, we
          might find both 171.69 (a 16-bit prefix) and 171.69.10 (a 24-bit
          prefix) in the forwarding table of a single router. In this case, a
          packet destined to, say, 171.69.10.5 clearly matches both prefixes.
          The rule in this case is based on the principle of “longest match”;
          that is, the packet matches the longest prefix, which would be
          171.69.10 in this example. On the other hand, a packet destined to
          171.69.20.5 would match 171.69 and not 171.69.10, and in the absence
          of any other matching entry in the routing table 171.69 would be the
          longest match.
        </p>
        <p>
          The task of efficiently finding the longest match between an IP
          address and the variable-length prefixes in a forwarding table has
          been a fruitful field of research for many years. The most well-known
          algorithm uses an approach known as a PATRICIA tree, which was
          actually developed well in advance of CIDR.
        </p>
      </section>
      <section>
        <h2>3.3.6 Address Translation (ARP)</h2>
        <p>
          In the previous section we talked about how to get IP datagrams to the
          right physical network but glossed over the issue of how to get a
          datagram to a particular host or router on that network. The main
          issue is that IP datagrams contain IP addresses, but the physical
          interface hardware on the host or router to which you want to send the
          datagram only understands the addressing scheme of that particular
          network. Thus, we need to translate the IP address to a link-level
          address that makes sense on this network (e.g., a 48-bit Ethernet
          address). We can then encapsulate the IP datagram inside a frame that
          contains that link-level address and send it either to the ultimate
          destination or to a router that promises to forward the datagram
          toward the ultimate destination.
        </p>
        <p>
          One simple way to map an IP address into a physical network address is
          to encode a host’s physical address in the host part of its IP
          address. For example, a host with physical address 00100001 01001001
          (which has the decimal value 33 in the upper byte and 81 in the lower
          byte) might be given the IP address 128. 96.33.81. Whilethis solution
          has been used on some networks, it is limited in that the network’s
          physical addresses can be no more than 16 bits long in this example;
          they can be only 8 bits long on a class C network. This clearly will
          not work for 48-bit Ethernet addresses.
        </p>
        <p>
          A more general solution would be for each host to maintain a table of
          address pairs; that is, the table would map IP addresses into physical
          addresses. While this table could be centrally managed by a system
          administrator and then copied to each host on the network, a better
          approach would be for each host to dynamically learn the contents of
          the table using the network. This can be accomplished using the
          Address Resolution Protocol (ARP). The goal of ARP is to enable each
          host on a network to build up a table of mappings between IP addresses
          and link-level addresses. Since these mappings may change over time
          (e.g., because an Ethernet card in a host breaks and is replaced by a
          new one with a new address), the entries are timed out periodically
          and removed. This happens on the order of every 15 minutes. The set of
          mappings currently stored in a host is known as the ARP cache or ARP
          table.
        </p>
        <p>
          ARPtakes advantage of the fact that many link-level network
          technologies, such as Ethernet, support broad cast. If a host wants to
          send an IP datagram to a host (or router) that it knows to be on the
          same network (i.e., the sending and receiving nodes have the same IP
          network number), it first checks for a mapping in the cache. If no
          mapping is found, it needs to invoke the Address Resolution Protocol
          over the network. It does this by broadcasting an ARP query onto the
          network. This query contains the IP address in question (the target IP
          address). Each host receives the query and checks to see if it matches
          its IP address. If it does match, the host sends a response message
          that contains its link-layer address back to the originator of the
          query. The originator adds the information contained in this response
          to its ARP table.
        </p>
        <p>
          The query message also includes the IP address and link-layer address
          of the sending host. Thus, when a host broadcasts a query message,
          each host on the network can learn the sender’s link-level and IP
          addresses and place that information in its ARP table. However, not
          every host adds this information to its ARP table. If the host already
          has an entry for that host in its table, it “refreshes” this entry;
          that is, it resets the length of time until it discards the entry. If
          that host is the target of the query, then it adds the information
          about the sender to its table, even if it did not already have an
          entry for that host. This is because there is a good chance that the
          source host is about to send it an application-level message, and it
          may eventually have to send a response or ACK back to the source; it
          will need the source’s physical address to do this. If a host is not
          the target and does not already have an entry for the source in its
          ARP table, then it does not add an entry for the source. This is
          because there is no reason to believe that this host will ever need
          the source’s link-level address; there is no need to clutter its ARP
          table with this information.
        </p>
        <p>
          Figure 3.24 shows the ARP packet format for IP-to-Ethernet address
          mappings. In fact, ARP can be used for lots of other kinds of
          mappings—the major differences are in the address sizes. In addition
          to the IP and link-layer addresses of both sender and target, the
          packet contains
        </p>
        <ul>
          <li>
            <strong>A</strong> HardwareTypefield, which specifies the type of
            physical network (e.g., Ethernet)
          </li>
          <li>
            <strong>A</strong> ProtocolTypefield, which specifies the
            higher-layer protocol (e.g., IP)
          </li>
          <li>
            <em>HLen</em> (“hardware” address length) and PLen (“protocol”
            address length) fields, which specify the length of the link-layer
            address and higher-layer protocol address, respectively
          </li>
          <li>
            <strong>An</strong> <em>Operation</em>, which specifies whether this
            is a request or a response
          </li>
          <li>
            The source and target hardware (Ethernet) and protocol (IP)
            addresses
          </li>
        </ul>
        <figure>
          <img
            src="images/fig3-24.PNG"
            alt="ARP packet format for mapping IP to Ethernet addresses"
            class="styled-image"
          />
          <figcaption>
            Figure 3.24: ARP packet format for mapping IP addresses into
            Ethernet addresses.
          </figcaption>
        </figure>
        <p>
          Note that the results of the ARP process can be added as an extra
          column in a forwarding table like the one in Table 3.6. Thus, for
          example, when R2 needs to forward a packet to network 2, it not only
          finds that the next hop is R1, but also finds the MAC address to place
          on the packet to send it to R1.
        </p>
        <p></p>
        <hr />
        <p><strong>Key Takeaway</strong></p>
        <p>
          We have now seen the basic mechanisms that IP provides for dealing
          with both heterogeneity and scale. On the issue of heterogeneity, IP
          begins by defining a best-effort service model that makes minimal
          assumptions about the underlying networks; most notably, this service
          model is based on unreliable datagrams. IP then makestwoimportant
          additions to this starting point: (1) a common packet format
          (fragmentation/reassembly is the mechanism that makes this format work
          over networks with different MTUs) and (2) a global address space for
          identifying all hosts (ARP is the mechanism that makes this global
          address space work over net works with different physical addressing
          schemes). On the issue of scale, IP uses hierarchical aggregation to
          reduce the amount of information needed to forward packets.
          Specifically, IP addresses are partitioned into network and host
          components, with packets first routed toward the destination network
          and then delivered to the correct host on that network. [Next]
        </p>
        <hr />
      </section>
      <section>
        <h2>3.3.7 Host Configuration (DHCP)</h2>
        <p>
          Ethernet addresses are configured into the network adaptor by the
          manufacturer, and this process is managed in such a way to ensure that
          these addresses are globally unique. This is clearly a sufficient
          condition to ensure that any collection of hosts connected to a single
          Ethernet (including an extended LAN) will have unique addresses.
          Furthermore, uniqueness is all we ask of Ethernet addresses.
        </p>
        <p>
          IP addresses, by contrast, not only must be unique on a given
          internetwork but also must reflect the structure of the internetwork.
          As noted above, they contain a network part and a host part, and the
          network part must be the same for all hosts on the same network. Thus,
          it is not possible for the IP address to be configured once into a
          host when it is manufactured, since that would imply that the
          manufacturer knew which hosts were going to end up on which networks,
          and it would mean that a host, once connected to one network, could
          never move to another. For this reason, IP addresses need to be
          reconfigurable.
        </p>
        <p>
          In addition to an IP address, there are some other pieces of
          information a host needs to have before it can start sending packets.
          The most notable of these is the address of a default router—the place
          to which it can send packets whose destination address is not on the
          same network as the sending host.
        </p>
        <p>
          Most host operating systems provide a way for a system administrator,
          or even a user, to manually configure the IP information needed by a
          host; however, there are some obvious drawbacks to such manual configu
          ration. One is that it is simply a lot of work to configure all the
          hosts in a large network directly, especially when you consider that
          such hosts are not reachable over a network until they are configured.
          Even more importantly, the configuration process is very error prone,
          since it is necessary to ensure that every host gets the correct
          network number and that no two hosts receive the same IP address. For
          these reasons, auto mated configuration methods are required. The
          primary method uses a protocol known as the Dynamic Host Configuration
          Protocol (DHCP).
        </p>
        <p>
          DHCPrelies on the existence of a DHCP server that is responsible for
          providing configuration information to hosts. There is at least one
          DHCP server for an administrative domain. At the simplest level, the
          DHCP server can function just as a centralized repository for host
          configuration information. Consider, for example, the problem of
          administering addresses in the internetwork of a large company. DHCP
          saves the network administrators from having to walk around to every
          host in the company with a list of addresses and network map in hand
          and configuring each host manually. Instead, the configuration
          information for each host could be stored in the DHCP server and
          automatically retrieved by each host when it is booted or connected to
          the network. However, the administrator would still pick the address
          that each host is to receive; he would just store that in the server.
          In this model, the configuration information for each host is stored
          in a table that is indexed by some form of unique client identifier,
          typically the hardware address (e.g., the Ethernet address of its
          network adaptor).
        </p>
        <p>
          A more sophisticated use of DHCP saves the network administrator from
          even having to assign addresses to individual hosts. In this model,
          the DHCP server maintains a pool of available addresses that it hands
          out to hosts on demand. This considerably reduces the amount of
          configuration an administrator must do, since now it is only necessary
          to allocate a range of IP addresses (all with the same network number)
          to each network.
        </p>
        <p>
          Since the goal of DHCP is to minimize the amount of manual
          configuration required for a host to function, it would rather defeat
          the purpose if each host had to be configured with the address of a
          DHCP server. Thus, the first problem faced by DHCP is that of server
          discovery.
        </p>
        <p>
          To contact a DHCP server, a newly booted or attached host sends a
          DHCPDISCOVER message to a special IP address (255.255.255.255) that is
          an IP broadcast address. This means it will be received by all hosts
          and routers on that network. (Routers do not forward such packets onto
          other networks, preventing broadcast to the entire Internet.) In the
          simplest case, one of these nodes is the DHCP server for the network.
          The server would then reply to the host that generated the discovery
          message (all the other nodes would ignore it). However, it is not
          really desirable to require one DHCP server on every network, because
          this still creates a potentially large number of servers that need to
          be correctly and consistently configured. Thus, DHCP uses the concept
          of a relay agent. There is at least one relay agent on each network,
          and it is configured with just one piece of information: the IP
          address of the DHCP server. When a relay agent receives a DHCPDISCOVER
          message, it unicasts it to the DHCP server and awaits the response,
          which it will then send back to the requesting client. The process of
          relaying a message from a host to a remote DHCP server is shown in
          Figure 3.25.
        </p>
        <p>
          Figure 3.26 below shows the format of a DHCP message. The message is
          actually sent using a protocol called the User Datagram Protocol (UDP)
          that runs over IP. UDP is discussed in detail in the next chapter, but
          the only interesting thing it does in this context is to provide a
          demultiplexing key that says, “This is a DHCPpacket.”
        </p>
        <figure>
          <img
            src="images/fig3-25.PNG"
            alt="DHCP relay agent forwarding DHCPDISCOVER message"
            class="styled-image"
          />
          <figcaption>
            Figure 3.25: A DHCP relay agent receives a broadcast DHCPDISCOVER
            message from a host and sends a unicast DHCPDISCOVER to the DHCP
            server.
          </figcaption>
        </figure>
        <p>
          DHCPis derived from an earlier protocol called BOOTP, and some of the
          packet fields are thus not strictly relevant to host configuration.
          When trying to obtain configuration information, the client puts its
          hardware address (e.g., its Ethernet address) in the chaddr field. The
          DHCP server replies by filling in the yiaddr (“your” IP address) field
          and sending it to the client. Other information such as the default
          router to be used by this client can be included in the options field.
        </p>
        <p>
          In the case where DHCP dynamically assigns IP addresses to hosts, it
          is clear that hosts cannot keep ad dresses indefinitely, as this would
          eventually cause the server to exhaust its address pool. At the same
          time, a host cannot be depended upon to give back its address, since
          it might have crashed, been unplugged from the network, or been turned
          off. Thus, DHCP allows addresses to be leased for some period of time.
          Once the lease expires, the server is free to return that address to
          its pool. A host with a leased address clearly needs to renew the
          lease periodically if in fact it is still connected to the network and
          functioning correctly.
        </p>
        <hr />
        <p><strong>Key Takeaway</strong></p>
        <p>
          DHCP illustrates an important aspect of scaling: the scaling of
          network management. While discussions of scaling often focus on
          keeping the state in network devices from growing too fast, it is
          important to pay attention to the growth of network management
          complexity. By allowing network managers to configure a range of IP
          addresses per network rather than one IP address per host, DHCP
          improves the manageability of a network. [Next]
        </p>
        <hr />
        <p>
          Note that DHCP may also introduce some more complexity into network
          management, since it makes the binding between physical hosts and IP
          addresses much more dynamic. This may make the network man ager’s job
          more difficult if, for example, it becomes necessary to locate a
          malfunctioning host.
        </p>
        <figure>
          <img
            src="images/fig3-26.PNG"
            alt="DHCP packet format"
            class="styled-image"
          />
          <figcaption>Figure 3.26: DHCP packet format.</figcaption>
        </figure>
      </section>
      <section>
        <h2>3.3.8 Error Reporting (ICMP)</h2>
        <p>
          The next issue is how the Internet treats errors. While IP is
          perfectly willing to drop datagrams when the going gets tough—for
          example, when a router does not know how to forward the datagram or
          when one fragment of a datagram fails to arrive at the destination—it
          does not necessarily fail silently. IP is always configured with a
          companion protocol, known as the Internet Control Message Protocol
          (ICMP), that defines a collection of error messages that are sent back
          to the source host whenever a router or host is unable to process an
          IP datagram successfully. For example, ICMP defines error messages
          indicating that the destination host is unreachable (perhaps due to a
          link failure), that the reassembly process failed, that the TTL had
          reached 0, that the IP header checksum failed, and so on.
        </p>
        <p>
          ICMP also defines a handful of control messages that a router can send
          back to a source host. One of the most useful control messages, called
          an ICMP-Redirect, tells the source host that there is a better route
          to the destination. ICMP-Redirects are used in the following
          situation. Suppose a host is connected to a network that has two
          routers attached to it, called R1 and R2, where the host uses R1 as
          its default router. Should R1 ever receive a datagram from the host,
          where based on its forwarding table it knows that R2 would have been a
          better choice for a particular destination address, it sends an
          ICMP-Redirect back to the host, instructing it to use R2 for all
          future datagrams addressed to that destination. The host then adds
          this new route to its forwarding table.
        </p>
        <p>
          ICMP also provides the basis for two widely used debugging tools, ping
          and traceroute. ping uses ICMP echo messages to determine if a node is
          reachable and alive. traceroute uses a slightly non intuitive
          technique to determine the set of routers along the path to a
          destination, which is the topic for one of the exercises at the end of
          this chapter.
        </p>
      </section>
      <section>
        <h2>3.3.9 Virtual Networks and Tunnels</h2>
        <p>
          We conclude our introduction to IP by considering an issue you might
          not have anticipated, but one that is increasingly important. Our
          discussion up to this point has focused on making it possible for
          nodes on different networks to communicate with each other in an
          unrestricted way. This is usually the goal in the Internet—everybody
          wants to be able to send email to everybody, and the creator of a new
          website wants to reach the widest possible audience. However, there
          are many situations where more controlled connectivity is required. An
          important example of such a situation is the virtual private network
          (VPN).
        </p>
        <p>
          The term VPN is heavily overused and definitions vary, but intuitively
          we can define a VPN by considering first the idea of a private
          network. Corporations with many sites often build private networks by
          leasing circuits from the phone companies and using those lines to
          interconnect sites. In such a network, com munication is restricted to
          take place only among the sites of that corporation, which is often
          desirable for security reasons. To make a private network virtual, the
          leased transmission lines—which are not shared with any other
          corporations—would be replaced by some sort of shared network. A
          virtual circuit (VC) is a very reasonable replacement for a leased
          line because it still provides a logical point-to-point connection
          between the corporation’s sites. For example, if corporation X has a
          VC from site A to site B, then clearly it can send packets between
          sites A and B. But there is no way that corporation Y can get its
          packets de livered to site B without first establishing its own
          virtual circuit to site B, and the establishment of such a VC can be
          administratively prevented, thus preventing unwanted connectivity
          between corporation X and corporation Y.
        </p>
        <p>
          Figure 3.27(a) shows two private networks for two separate
          corporations. In Figure 3.27(b) they are both migrated to a virtual
          circuit network. The limited connectivity of a real private network is
          maintained, but since the private networks now share the same
          transmission facilities and switches we say that two virtual private
          networks have been created.
        </p>
        <p>
          In Figure 3.27, a virtual circuit network (using ATM, for example) is
          used to provide the controlled con nectivity among sites. It is also
          possible to provide a similar function using an IP network to provide
          the connectivity. However, we cannot just connect the various
          corporations’ sites to a single internetwork be cause that would
          provide connectivity between corporation X and corporation Y, which we
          wish to avoid. To solve this problem, we need to introduce a new
          concept, the IP tunnel.
        </p>
        <p>
          We can think of an IPtunnel as a virtual point-to-point link between a
          pair of nodes that are actually separated by an arbitrary number of
          networks. The virtual link is created within the router at the
          entrance to the tunnel by providing it with the IP address of the
          router at the far end of the tunnel. Whenever the router at the
          entrance of the tunnel wants to send a packet over this virtual link,
          it encapsulates the packet inside an IP datagram. The destination
          address in the IP header is the address of the router at the far end
          of the tunnel, while the source address is that of the encapsulating
          router.
        </p>
        <p>
          In the forwarding table of the router at the entrance to the tunnel,
          this virtual link looks much like a normal link. Consider, for
          example, the network in Figure 3.28. A tunnel has been configured from
          R1 to R2 and assigned a virtual interface number of 0. The forwarding
          table in R1 might therefore look like Table 3.9.
        </p>
        <table aria-labelledby="table-3-9">
          <caption id="table-3-9">
            <strong>Table 3.9:</strong>
            Forwarding Table for Router R1.
          </caption>
          <thead>
            <tr>
              <th>NetworkNum</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>Interface 0</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Virtual interface 0</td>
            </tr>
            <tr>
              <td>Default</td>
              <td>Interface 1</td>
            </tr>
          </tbody>
        </table>
        <figure>
          <img
            src="images/fig3-27.PNG"
            alt="Example of virtual private networks with shared and separate switches"
            class="styled-image"
          />
          <figcaption>
            Figure 3.27: An example of virtual private networks: (a) two
            separate private networks; (b) two virtual private networks sharing
            common switches.
          </figcaption>
        </figure>
        <figure>
          <img
            src="images/fig3-28.PNG"
            alt="Tunnel through an internetwork from R1 to R2 at 18.5.0.1"
            class="styled-image"
          />
          <figcaption>
            Figure 3.28: A tunnel through an internetwork. 18.5.0.1 is the
            address of R2 that can be reached from R1 across the internetwork.
          </figcaption>
        </figure>

        <p>
          R1 has two physical interfaces. Interface 0 connects to network 1;
          interface 1 connects to a large internet work and is thus the default
          for all traffic that does not match something more specific in the
          forwarding table. In addition, R1 has a virtual interface, which is
          the interface to the tunnel. Suppose R1 receives a packet from network
          1 that contains an address in network 2. The forwarding table says
          this packet should be sent out virtual interface 0. In order to send a
          packet out this interface, the router takes the packet, adds an IP
          header addressed to R2, and then proceeds to forward the packet as if
          it had just been received. R2’s address is 18.5.0.1; since the network
          number of this address is 18, not 1 or 2, a packet destined for R2
          will be forwarded out the default interface into the internetwork.
        </p>
        <p>
          Once the packet leaves R1, it looks to the rest of the world like a
          normal IP packet destined to R2, and it is forwarded accordingly. All
          the routers in the internetwork forward it using normal means, until
          it arrives at R2. When R2 receives the packet, it finds that it
          carries its own address, so it removes the IP header and looks at the
          payload of the packet. What it finds is an inner IP packet whose
          destination address is in network 2. R2 now processes this packet like
          any other IP packet it receives. Since R2 is directly connected to
          network 2, it forwards the packet on to that network. Figur 3.28 shows
          the change in encapsulation of the packet as it moves across the
          network.
        </p>
        <p>
          While R2 is acting as the endpoint of the tunnel, there is nothing to
          prevent it from performing the normal functions of a router. For
          example, it might receive some packets that are not tunneled, but that
          are addressed to networks that it knows how to reach, and it would
          forward them in the normal way.
        </p>
        <p>
          You might wonder why anyone would want to go to all the trouble of
          creating a tunnel and changing the encapsulation of a packet as it
          goes across an internetwork. One reason is security. Supplemented with
          encryption, a tunnel can become a very private sort of link across a
          public network. Another reason may be that R1 and R2 have some
          capabilities that are not widely available in the intervening
          networks, such as multicast routing. By connecting these routers with
          a tunnel, we can build a virtual network in which all the routers with
          this capability appear to be directly connected. A third reason to
          build tunnels is to carry packets from protocols other than IP across
          an IP network. As long as the routers at either end of the tunnel know
          how to handle these other protocols, the IP tunnel looks to them like
          a point-to-point link over which they can send non-IP packets. Tunnels
          also provide a mechanism by which we can force a packet to be
          delivered to a particular place even if its original header—the one
          that gets encapsulated inside the tunnel header—might suggest that it
          should go somewhere else. Thus, we see that tunneling is a powerful
          and quite general technique for building virtual links across
          internetworks. So general, in fact, that the technique recurses, with
          the most common use case being to tunnel IP over IP.
        </p>
        <p>
          Tunneling does have its downsides. One is that it increases the length
          of packets; this might represent a significant waste of bandwidth for
          short packets. Longer packets might be subject to fragmentation, which
          has its own set of drawbacks. There may also be performance
          implications for the routers at either end of the tunnel, since they
          need to do more work than normal forwarding as they add and remove the
          tunnel header. Finally, there is a management cost for the
          administrative entity that is responsible for setting up the tunnels
          and making sure they are correctly handled by the routing protocols.
        </p>
      </section>
      <section>
        <h2>3.4 Routing</h2>
        <p>
          So far in this chapter we have assumed that the switches and routers
          have enough knowledge of the network topology so they can choose the
          right port onto which each packet should be output. In the case of
          virtual circuits, routing is an issue only for the connection request
          packet; all subsequent packets follow the same path as the request. In
          datagram networks, including IP networks, routing is an issue for
          every packet. In either case, a switch or router needs to be able to
          look at a destination address and then to determine which of the
          output ports is the best choice to get a packet to that address. As we
          saw in an earlier section, the switch makes this decision by
          consulting a forwarding table. The fundamental problem of routing is
          how switches and routers acquire the information in their forwarding
          tables.
        </p>
        <hr />
        <p><strong>Key Takeaway</strong></p>
        <p>
          We restate an important distinction, which is often neglected, between
          forwarding and routing. Forwarding consists of receiving a packet,
          looking up its destination address in a table, and sending the packet
          in a direction determined by that table. We saw several examples of
          forwarding in the preceding section. It is a simple and well-defined
          process performed locally at each node, and is often referred to as
          the network’s data plane. Routing is the process by which forwarding
          tables are built. It depends on complex distributed algorithms, and is
          often referred to as the network’s control plane. [Next]
        </p>
        <hr />
        <p>
          While the terms forwarding table and routing table are sometimes used
          interchangeably, we will make a distinction between them here. The
          forwarding table is used when a packet is being forwarded and so must
          contain enough information to accomplish the forwarding function. This
          means that a row in the forwarding table contains the mapping from a
          network prefix to an outgoing interface and some MAC information, such
          as the Ethernet address of the next hop. The routing table, on the
          other hand, is the table that is built up by the routing algorithms as
          a precursor to building the forwarding table. It generally contains
          mappings from network prefixes to next hops. It may also contain
          information about how this information was learned, so that the router
          will be able to decide when it should discard some information.
        </p>
        <p>
          Whether the routing table and forwarding table are actually separate
          data structures is something of an implementation choice, but there
          are numerous reasons to keep them separate. For example, the
          forwarding table needs to be structured to optimize the process of
          looking up an address when forwarding a packet, while the routing
          table needs to be optimized for the purpose of calculating changes in
          topology. In many cases, the forwarding table may even be implemented
          in specialized hardware, whereas this is rarely if ever done for the
          routing table.
        </p>
        <p>
          Table 3.10 gives an example of a row from a routing table, which tells
          us that network prefix 18/8 is to be reached by a next hop router with
          the IP address 171.69.245.10
        </p>
        <table aria-labelledby="table-3-10">
          <caption id="table-3-10">
            <strong>Table 3.10:</strong>
            Example row from a routing table.
          </caption>
          <thead>
            <tr>
              <th>Prefix/Length</th>
              <th>Next Hop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>18/8</td>
              <td>171.69.245.10</td>
            </tr>
          </tbody>
        </table>
        <p>
          In contrast, Table 3.11 gives an example of a row from a forwarding
          table, which contains the information about exactly how to forward a
          packet to that next hop: Send it out interface number 0 with a MAC
          address of 8:0:2b:e4:b:1:2. Note that the last piece of information is
          provided by the Address Resolution Protocol.
        </p>
        <table aria-labelledby="table-3-11">
          <caption id="table-3-11">
            <strong>Table 3.11:</strong>
            Example row from a forwarding table.
          </caption>
          <thead>
            <tr>
              <th>Prefix/Length</th>
              <th>Interface</th>
              <th>MAC Address</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>18/8</td>
              <td>if0</td>
              <td>8:0:2b:e4:b:1:2</td>
            </tr>
          </tbody>
        </table>
        <p>
          Before getting into the details of routing, we need to remind
          ourselves of the key question we should be asking anytime we try to
          build a mechanism for the Internet: “Does this solution scale?” The
          answer for the algorithms and protocols described in this section is
          “not so much.” They are designed for networks of fairly modest size—up
          to a few hundred nodes, in practice. However, the solutions we
          describe do serve as a building block for a hierarchical routing
          infrastructure that is used in the Internet today. Specifically, the
          protocols described in this section are collectively known as
          intradomain routing protocols, or interior gateway protocols (IGPs).
          To understand these terms, we need to define a routing domain. A good
          working definition is an internetwork in which all the routers are
          under the same administrative control (e.g., a single university
          campus, or the network of a single Internet Service Provider). The
          relevance of this definition will become apparent in the next chapter
          when we look at interdomain routing protocols. For now, the important
          thing to keep in mind is that we are considering the problem of
          routing in the context of small to midsized networks, not for a
          network the size of the Internet.
        </p>
      </section>
      <section>
        <h2>3.4.1 Network as a Graph</h2>
        <p>
          Routing is, in essence, a problem of graph theory. Figure 3.29 shows a
          graph representing a network. The nodes of the graph, labeled A
          through F, may be hosts, switches, routers, or networks. For our
          initial discussion, we will focus on the case where the nodes are
          routers. The edges of the graph correspond to the network links. Each
          edge has an associated cost, which gives some indication of the
          desirability of sending traffic over that link. A discussion of how
          edge costs are assigned is given in a later section.
        </p>
        <p>
          Note that the example networks (graphs) used throughout this chapter
          have undirected edges that are as signed a single cost. This is
          actually a slight simplification. It is more accurate to make the
          edges directed, which typically means that there would be a pair of
          edges between each node—one flowingineachdirection, and each with its
          own edge cost.
        </p>
        <p>
          The basic problem of routing is to find the lowest-cost path between
          any two nodes, where the cost of a path equals the sum of the costs of
          all the edges that make up the path. For a simple network like the one
          in Figure 3.29, you could imagine just calculating all the shortest
          paths and loading them into some nonvolatile storage on each node.
          Such a static approach has several shortcomings:
        </p>
        <ul>
          <li>It does not deal with node or link failures.</li>
          <li>It does not consider the addition of new nodes or links.</li>
          <li>
            It implies that edge costs cannot change, even though we might
            reasonably wish to have link costs change over time (e.g., assigning
            high cost to a link that is heavily loaded).
          </li>
        </ul>
        <p>
          For these reasons, routing is achieved in most practical networks by
          running routing protocols among the nodes. These protocols provide a
          distributed, dynamic way to solve the problem of finding the lowest
          cost path in the presence of link and node failures and changing edge
          costs. Note the word distributed in the previous sentence; it is
          difficult to make centralized solutions scalable, so all the widely
          used routing protocols use distributed algorithms.
        </p>
        <p>
          The distributed nature of routing algorithms is one of the main
          reasons why this has been such a rich field of research and
          development—there are a lot of challenges in making distributed
          algorithms work well. For example, distributed algorithms raise the
          possibility that two routers will at one instant have different ideas
          about the shortest path to some destination. In fact, each one may
          think that the other one is closer to the destination and decide to
          send packets to the other one. Clearly, such packets will be stuck in
          a loop until the discrepancy between the two routers is resolved, and
          it would be good to resolve it as soon as possible. This is just one
          example of the type of problem routing protocols must address.
        </p>
        <p>
          To begin our analysis, we assume that the edge costs in the network
          are known. We will examine the two main classes of routing protocols:
          distance vector and link state. In a later section, we return to the
          problem of calculating edge costs in a meaningful way.
        </p>
      </section>
      <section>
        <h2>3.4.2 Distance-Vector (RIP)</h2>
        <p>
          The idea behind the distance-vector algorithm is suggested by its
          name. (The other common name for this class of algorithm is
          Bellman-Ford, after its inventors.) Each node constructs a
          one-dimensional array (a vector) containing the “distances” (costs) to
          all other nodes and distributes that vector to its immediate
          neighbors. The starting assumption for distance-vector routing is that
          each node knows the cost of the link to each of its directly connected
          neighbors. These costs may be provided when the router is configured
          by a network manager. A link that is down is assigned an infinite
          cost.
        </p>
        <figure>
          <img
            src="images/fig3-30.PNG"
            alt="Distance-vector routing example network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.30: Distance-vector routing: an example network.
          </figcaption>
        </figure>
        <table aria-labelledby="table-3-12">
          <caption id="table-3-12">
            <strong>Table 3.12:</strong>
            Initial Distances Stored at Each Node (Global View).
          </caption>
          <thead>
            <tr>
              <th></th>
              <th>A</th>
              <th>B</th>
              <th>C</th>
              <th>D</th>
              <th>E</th>
              <th>F</th>
              <th>G</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">A</th>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>∞</td>
              <td>1</td>
              <td>1</td>
              <td>∞</td>
            </tr>
            <tr>
              <th scope="row">B</th>
              <td>1</td>
              <td>0</td>
              <td>1</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
            </tr>
            <tr>
              <th scope="row">C</th>
              <td>1</td>
              <td>1</td>
              <td>0</td>
              <td>1</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
            </tr>
            <tr>
              <th scope="row">D</th>
              <td>∞</td>
              <td>∞</td>
              <td>1</td>
              <td>0</td>
              <td>∞</td>
              <td>∞</td>
              <td>1</td>
            </tr>
            <tr>
              <th scope="row">E</th>
              <td>1</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
              <td>0</td>
              <td>∞</td>
              <td>∞</td>
            </tr>
            <tr>
              <th scope="row">F</th>
              <td>1</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th scope="row">G</th>
              <td>∞</td>
              <td>∞</td>
              <td>∞</td>
              <td>1</td>
              <td>∞</td>
              <td>1</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>
          To see how a distance-vector routing algorithm works, it is easiest to
          consider an example like the one depicted in Figure 3.30. In this
          example, the cost of each link is set to 1, so that a least-cost path
          is simply the one with the fewest hops. (Since all edges have the same
          cost, we do not show the costs in the graph.) We can represent each
          node’s knowledge about the distances to all other nodes as a table
          like Table 3.12. Note that each node knows only the information in one
          row of the table (the one that bears its name in the left column). The
          global view that is presented here is not available at any single
          point in the network.
        </p>
        <p>
          We may consider each row in Table 3.12 as a list of distances from one
          node to all other nodes, representing the current beliefs of that
          node. Initially, each node sets a cost of 1 to its directly connected
          neighbors and ∞toall other nodes. Thus, A initially believes that it
          can reach B in one hop and that D is unreachable. The routing table
          stored at A reflects this set of beliefs and includes the name of the
          next hop that A would use to reach any reachable node. Initially,
          then, A’s routing table would look like Table 3.13.
        </p>
        <table aria-labelledby="table-3-13">
          <caption id="table-3-13">
            <strong>Table 3.13:</strong>
            Initial Routing Table at Node A.
          </caption>
          <thead>
            <tr>
              <th>Destination</th>
              <th>Cost</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>B</td>
              <td>1</td>
              <td>C</td>
            </tr>
            <tr>
              <td>C</td>
              <td>1</td>
              <td>D</td>
            </tr>
            <tr>
              <td>D</td>
              <td>∞</td>
              <td>—</td>
            </tr>
            <tr>
              <td>E</td>
              <td>1</td>
              <td>F</td>
            </tr>
            <tr>
              <td>G</td>
              <td>∞</td>
              <td>—</td>
            </tr>
          </tbody>
        </table>
        <p>
          The next step in distance-vector routing is that every node sends a
          message to its directly connected neigh bors containing its personal
          list of distances. For example, node F tells node A that it can reach
          node G at a cost of 1; A also knows it can reach F at a cost of 1, so
          it adds these costs to get the cost of reaching G by means of F. This
          total cost of 2 is less than the current cost of infinity, so A
          records that it can reach G at a cost of 2 by going through F.
          Similarly, A learns from C that D can be reached from C at a cost of
          1; it adds this to the cost of reaching C (1) and decides that D can
          be reached via C at a cost of 2, which is better than the old cost of
          infinity. At the same time, A learns from C that B can be reached from
          C at a cost of 1, so it concludes that the cost of reaching B via C is
          2. Since this is worse than the current cost of reaching B (1), this
          new information is ignored. At this point, A can update its routing
          table with costs and next hops for all nodes in the network. The
          result is shown in Table 3.14.
        </p>
        <table aria-labelledby="table-3-14">
          <caption id="table-3-14">
            <strong>Table 3.14:</strong>
            Final Routing Table at Node A.
          </caption>
          <thead>
            <tr>
              <th>Destination</th>
              <th>Cost</th>
              <th>NextHop</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>B</td>
              <td>1</td>
              <td>C</td>
            </tr>
            <tr>
              <td>C</td>
              <td>1</td>
              <td>D</td>
            </tr>
            <tr>
              <td>D</td>
              <td>2</td>
              <td>C</td>
            </tr>
            <tr>
              <td>E</td>
              <td>1</td>
              <td>C</td>
            </tr>
            <tr>
              <td>F</td>
              <td>1</td>
              <td>E</td>
            </tr>
            <tr>
              <td>G</td>
              <td>2</td>
              <td>F</td>
            </tr>
          </tbody>
        </table>
        <p>
          In the absence of any topology changes, it takes only a few exchanges
          of information between neighbors before each node has a complete
          routing table. The process of getting consistent routing information
          to all the nodes is called convergence. Table 3.15 shows the final set
          of costs from each node to all other nodes when routing has converged.
          We must stress that there is no one node in the network that has all
          the information in this table—each node only knows about the contents
          of its own routing table. The beauty of a distributed algorithm like
          this is that it enables all nodes to achieve a consistent view of the
          network in the absence of any centralized authority.
        </p>
        <table aria-labelledby="table-3-15">
          <caption id="table-3-15">
            <strong>Table 3.15:</strong>
            Final Distances Stored at Each Node (Global View).
          </caption>
          <thead>
            <tr>
              <th></th>
              <th>A</th>
              <th>B</th>
              <th>C</th>
              <th>D</th>
              <th>E</th>
              <th>F</th>
              <th>G</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">A</th>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>2</td>
              <td>1</td>
              <td>1</td>
              <td>2</td>
            </tr>
            <tr>
              <th scope="row">B</th>
              <td>1</td>
              <td>0</td>
              <td>1</td>
              <td>2</td>
              <td>2</td>
              <td>2</td>
              <td>3</td>
            </tr>
            <tr>
              <th scope="row">C</th>
              <td>1</td>
              <td>1</td>
              <td>0</td>
              <td>1</td>
              <td>2</td>
              <td>2</td>
              <td>2</td>
            </tr>
            <tr>
              <th scope="row">D</th>
              <td>2</td>
              <td>2</td>
              <td>1</td>
              <td>0</td>
              <td>3</td>
              <td>2</td>
              <td>1</td>
            </tr>
            <tr>
              <th scope="row">E</th>
              <td>1</td>
              <td>2</td>
              <td>2</td>
              <td>3</td>
              <td>0</td>
              <td>2</td>
              <td>3</td>
            </tr>
            <tr>
              <th scope="row">F</th>
              <td>1</td>
              <td>2</td>
              <td>2</td>
              <td>2</td>
              <td>2</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th scope="row">G</th>
              <td>2</td>
              <td>3</td>
              <td>2</td>
              <td>1</td>
              <td>3</td>
              <td>1</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>
          There are a few details to fill in before our discussion of
          distance-vector routing is complete. First we note that there are two
          different circumstances under which a given node decides to send a
          routing update to its neighbors. One of these circumstances is the
          periodic update. In this case, each node automatically sends an update
          message every so often, even if nothing has changed. This serves to
          let the other nodes know that this node is still running. It also
          makes sure that they keep getting information that they may need if
          their current routes become unviable. The frequency of these periodic
          updates varies from protocol to protocol, but it is typically on the
          order of several seconds to several minutes. The second mechanism,
          sometimes called a triggered update, happens whenever a node notices a
          link failure or receives an update from one of its neighbors that
          causes it to change one of the routes in its routing table. Whenever a
          node’s routing table changes, it sends an update to its neighbors,
          which may lead to a change in their tables, causing them to send an
          update to their neighbors.
        </p>
        <p>
          Now consider what happens when a link or node fails. The nodes that
          notice first send new lists of distances to their neighbors, and
          normally the system settles down fairly quickly to a new state. As to
          the question of how a node detects a failure, there are a couple of
          different answers. In one approach, a node continually tests the link
          to another node by sending a control packet and seeing if it receives
          an acknowledgment. In another approach, a node determines that the
          link (or the node at the other end of the link) is down if it does not
          receive the expected periodic routing update for the last few update
          cycles.
        </p>
        <p>
          To understand what happens when a node detects a link failure,
          consider what happens when F detects that its link to G has failed.
          First, F sets its new distance to G to infinity and passes that
          information along to A. Since A knows that its 2-hop path to G is
          through F, A would also set its distance to G to infinity. However,
          with the next update from C, A would learn that C has a 2-hop path to
          G. Thus, A would know that it could reach G in 3 hops through C, which
          is less than infinity, and so A would update its table accordingly.
          When it advertises this to F, node F would learn that it can reach G
          at a cost of 4 through A, which is less than infinity, and the system
          would again become stable.
        </p>
        <p>
          Unfortunately, slightly different circumstances can prevent the
          network from stabilizing. Suppose, for ex ample, that the link from A
          to E goes down. In the next round of updates, A advertises a distance
          of infinity to E, but B and C advertise a distance of 2 to E.
          Depending on the exact timing of events, the following might happen:
          Node B, upon hearing that E can be reached in 2 hops from C, concludes
          that it can reach E in 3 hops and advertises this to A; node A
          concludes that it can reach E in 4 hops and advertises this to C; node
          C concludes that it can reach E in 5 hops; and so on. This cycle stops
          only when the distances reach some number that is large enough to be
          considered infinite. In the meantime, none of the nodes actually knows
          that E is unreachable, and the routing tables for the network do not
          stabilize. This situation is known as the count to infinity problem.
        </p>
        <p>
          There are several partial solutions to this problem. The first one is
          to use some relatively small number as an approximation of infinity.
          For example, we might decide that the maximum number of hops to get
          across a certain network is never going to be more than 16, and so we
          could pick 16 as the value that represents infinity. This at least
          bounds the amount of time that it takes to count to infinity. Of
          course, it could also present a problem if our network grew to a point
          where some nodes were separated by more than 16 hops.
        </p>
        <p>
          One technique to improve the time to stabilize routing is called split
          horizon. The idea is that when a node sends a routing update to its
          neighbors, it does not send those routes it learned from each neighbor
          back to that neighbor. For example, if B has the route (E, 2, A) in
          its table, then it knows it must have learned this route from A, and
          so whenever B sends a routing update to A, it does not include the
          route (E, 2) in that update. In a stronger variation of split horizon,
          called split horizon with poison reverse, B actually sends that route
          back to A, but it puts negative information in the route to ensure
          that A will not eventually use B to get to E. For example, B sends the
          route (E, ∞) to A. The problem with both of these techniques is that
          they only work for routing loops that involve two nodes. For larger
          routing loops, more drastic measures are called for. Continuing the
          above example, if B and C had waited for a while after hearing of the
          link failure from A before advertising routes to E, they would have
          found that neither of them really had a route to E. Unfortunately,
          this approach delays the convergence of the protocol; speed of
          convergence is one of the key advantages of its competitor, link-state
          routing, the subject of a later section.
        </p>
      </section>
      <section>
        <h2>Implementation</h2>
        <p>
          The code that implements this algorithm is very straight forward; we
          give only some of the basics here. Structure Route defines each entry
          in the routing table, and constantMAX_TTL specifies how long an entry
          is kept in the table before it is discarded.
        </p>
        <figure>
          <img
            src="images/code-img6.PNG"
            alt="C structure defining routing table entries for datagram forwarding"
            class="styled-image"
          />
          <figcaption>
            Figure: C structure defining a routing table for a datagram
            forwarding algorithm, including destination, next hop, cost, and
            time-to-live (TTL) fields.
          </figcaption>
        </figure>
        <p>
          The routine that updates the local node’s routing table based on a new
          route is given by mergeRoute. Although not shown,a timer function
          periodically scans the list of routes in the node’s routing table,
          decrements the TTL (timetolive) field of each route,and discards any
          routes that have a time to live of 0. Notice, however, that the TTL
          field is reset to MAX_TTL any time the route is reconfirmed by an
          update message from a neighboring node.
        </p>
        <figure>
          <img
            src="images/code-img7.PNG"
            alt="Function to merge routing entries in datagram forwarding algorithm"
            class="styled-image"
          />
          <figcaption>
            Figure: Function for merging routing entries in a datagram
            forwarding algorithm. It updates the routing table by comparing
            destination, cost, and next hop, and resets TTL when a new or
            improved route is found.
          </figcaption>
        </figure>
        <p>
          Finally, the procedure updateRoutingTableisthe mainroutine that calls
          mergeRoute to incorporate all the routes contained in a routing update
          that is received from a neighboring node.
        </p>
        <figure>
          <img
            src="images/code-img8.PNG"
            alt="Function to update routing table using mergeRoute logic"
            class="styled-image"
          />
          <figcaption>
            Figure: Function for updating the routing table in a datagram
            forwarding algorithm. It iterates through new routes and integrates
            them using the mergeRoute procedure.
          </figcaption>
        </figure>
      </section>
      <section>
        <h2>Routing Information Protocol (RIP)</h2>
        <p>
          One of the more widely used routing protocols in IP networks is the
          Routing Information Protocol (RIP). Its widespread use in the early
          days of IP was due in no small part to the fact that it was
          distributed along with the popular Berkeley Software Distribution
          (BSD) version of Unix, from which many commercial versions of Unix
          were derived. It is also extremely simple. RIP is the canonical
          example of a routing protocol built on the distance-vector algorithm
          just described.
        </p>
        <p>
          Routing protocols in internetworks differ very slightly from the
          idealized graph model described above. In an internetwork, the goal of
          the routers is to learn how to forward packets to various networks.
          Thus, rather than advertising the cost of reaching other routers, the
          routers advertise the cost of reaching networks. For example, in
          Figure 3.31, router C would advertise to router A the fact that it can
          reach networks 2 and 3 (to which it is directly connected) at a cost
          of 0, networks 5 and 6 at cost 1, and network 4 at cost 2.
        </p>
        <p>
          We can see evidence of this in the RIP (version 2) packet format in
          Figure 3.32. The majority of the packet is taken up with (address,
          mask, distance)triples. However,theprinciplesof therouting algorithm
          are just the same. For example, if router A learns from router B that
          network X can be reached at a lower cost via B than via the existing
          next hop in the routing table, A updates the cost and next hop
          information for the network number accordingly.
        </p>
        <p>
          RIP is in fact a fairly straightforward implementation of
          distance-vector routing. Routers running RIP send their advertisements
          every 30 seconds; a router also sends an update message whenever an
          update from another router causes it to change its routing table. One
          point of interest is that it supports multiple address
        </p>
        <figure>
          <img
            src="images/fig3-31.PNG"
            alt="Example network running RIP protocol"
            class="styled-image"
          />
          <figcaption>Figure 3.31: Example network running RIP.</figcaption>
        </figure>
        <figure>
          <img
            src="images/fig3-32.PNG"
            alt="RIPv2 packet format"
            class="styled-image"
          />
          <figcaption>Figure 3.32: RIPv2 packet format.</figcaption>
        </figure>

        <p>
          families, not just IP—that is the reason for the Family part of the
          advertisements. RIP version 2 (RIPv2) also introduced the subnet masks
          described in an earlier section, whereas RIP version 1 worked with the
          old classful addresses of IP.
        </p>
        <p>
          As we will see below, it is possible to use a range of different
          metrics or costs for the links in a routing protocol. RIP takes the
          simplest approach, with all link costs being equal to 1, just as in
          our example above. Thus, it always tries to find the minimum hop
          route. Valid distances are 1 through 15, with 16 representing
          infinity. This also limits RIP to running on fairly small
          networks—those with no paths longer than 15 hops.
        </p>
      </section>
      <section>
        <h2>3.4.3 Link State (OSPF)</h2>
        <p>
          Link-state routing is the second major class of intradomain routing
          protocol. The starting assumptions for link-state routing are rather
          similar to those for distance-vector routing. Each node is assumed to
          be capable of finding out the state of the link to its neighbors (up
          or down) and the cost of each link. Again, we want to provide each
          node with enough information to enable it to find the least-cost path
          to any destination. The basic idea behind link-state protocols is very
          simple: Every node knows how to reach its directly connected
          neighbors, and if we make sure that the totality of this knowledge is
          disseminated to every node, then every node will have enough knowledge
          of the network to build a complete map of the network. This is clearly
          a sufficient condition (although not a necessary one) for finding the
          shortest path to any point in the network. Thus, link-state routing
          protocols rely on two mechanisms: reliable dissemination of link-state
          information, and the calculation of routes from the sum of all the
          accumulated link-state knowledge.
        </p>
        <h2>Reliable Flooding</h2>
        <p>
          Reliable flooding is the process of making sure that all the nodes
          participating in the routing protocol get a copy of the link-state
          information from all the other nodes. As the term flooding suggests,
          the basic idea is for a node to send its link-state information out on
          all of its directly connected links; each node that receives this
          information then forwards it out on all of its links. This process
          continues until the information has reached all the nodes in the
          network.
        </p>
        <p>
          More precisely, each node creates an update packet, also called a
          link-state packet (LSP), which contains the following information:
        </p>
        <ul>
          <li>The ID of the node that created the LSP</li>
          <li>
            A list of directly connected neighbors of that node, with the cost
            of the link to each one
          </li>
          <li>A sequence number</li>
          <li>A time to live for this packet</li>
        </ul>
        <p>
          The first two items are needed to enable route calculation; the last
          two are used to make the process of f looding the packet to all nodes
          reliable. Reliability includes making sure that you have the most
          recent copy of the information, since there may be multiple,
          contradictory LSPs from one node traversing the network. Making the
          flooding reliable has proven to be quite difficult. (For example, an
          early version of link-state routing used in the ARPANET caused that
          network to fail in 1981.)
        </p>
        <p>
          Flooding works in the following way. First, the transmission of LSPs
          between adjacent routers is made reliable using acknowledgments and
          retransmissions just as in the reliable link-layer protocol. However,
          several more steps are necessary to reliably flood an LSP to all nodes
          in a network. Consider a node X that receives a copy of an LSP that
          originated at some other node Y. Note that Y may be any other router
          in the same routing domain as X. X checks to see if it has already
          stored a copy of an LSP from Y. If not, it stores the LSP. If it
          already has a copy, it compares the sequence numbers; if the new LSP
          has a larger sequence number, it is assumed to be the more recent, and
          that LSP is stored, replacing the old one. A smaller (or equal)
          sequence number would imply an LSP older (or not newer) than the one
          stored, so it would be discarded and no further action would be
          needed. If the received LSP was the newer one, X then sends a copy of
          that LSP to all of its neighbors except the neighbor from which the
          LSP was just received. The fact that the LSP is not sent back to the
          node from which it was received helps to bring an end to the f looding
          of an LSP. Since X passes the LSP on to all its neighbors, who then
          turn around and do the same thing, the most recent copy of the LSP
          eventually reaches all nodes.
        </p>
        <figure>
          <img
            src="images/fig3-33.PNG"
            alt="Flooding of link-state packets across nodes X, A, C, and B"
            class="styled-image"
          />
          <figcaption>
            Figure 3.33: Flooding of link-state packets: (a) LSP arrives at node
            X; (b) X floods LSP to A and C; (c) A and C flood LSP to B (but not
            X); (d) flooding is complete.
          </figcaption>
        </figure>
        <p>
          Figure 3.33 shows an LSP being flooded in a small network. Each node
          becomes shaded as it stores the new LSP. In Figure 3.33(a) the LSP
          arrives at node X, which sends it to neighbors A and C in Figure
          3.33(b). A and C do not send it back to X, but send it on to B. Since
          B receives two identical copies of the LSP, it will accept whichever
          arrived first and ignore the second as a duplicate. It then passes the
          LSP onto D, which has no neighbors to flood it to, and the process is
          complete.
        </p>
        <p>
          Just as in RIP, each node generates LSPs under two circumstances.
          Either the expiry of a periodic timer or a change in topology can
          cause a node to generate a new LSP. However, the only topology-based
          reason for a node to generate an LSP is if one of its directly
          connected links or immediate neighbors has gone down. The failure of a
          link can be detected in some cases by the link-layer protocol. The
          demise of a neighbor or loss of connectivity to that neighbor can be
          detected using periodic “hello” packets. Each node sends these to its
          immediate neighbors at defined intervals. If a sufficiently long time
          passes without receipt of a “hello” from a neighbor, the link to that
          neighbor will be declared down, and a new LSP will be generated to
          reflect this fact.
        </p>
        <p>
          One of the important design goals of a link-state protocol’s flooding
          mechanism is that the newest infor mation must be flooded to all nodes
          as quickly as possible, while old information must be removed from the
          network and not allowed to circulate. In addition, it is clearly
          desirable to minimize the total amount of routing traffic that is sent
          around the network; after all, this is just overhead from the
          perspective of those whoactually use the network for their
          applications. The next few paragraphs describe some of the ways that
          these goals are accomplished.
        </p>
        <p>
          One easy way to reduce overhead is to avoid generating LSPs unless
          absolutely necessary. This can be done by using very long timers—often
          on the order of hours—for the periodic generation of LSPs. Given that
          the f looding protocol is truly reliable when topology changes, it is
          safe to assume that messages saying “nothing has changed” do not need
          to be sent very often.
        </p>
        <p>
          To make sure that old information is replaced by newer information,
          LSPs carry sequence numbers. Each time a node generates a new LSP, it
          increments the sequence number by 1. Unlike most sequence numbers used
          in protocols, these sequence numbers are not expected to wrap, so the
          field needs to be quite large (say, 64 bits). If a node goes down and
          then comes back up, it starts with a sequence number of 0. If the node
          was down for a long time, all the old LSPs for that node will have
          timed out (as described below); otherwise, this node will eventually
          receive a copy of its own LSP with a higher sequence number, which it
          can then increment and use as its own sequence number. This will
          ensure that its new LSP replaces any of its old LSPs left over from
          before the node went down.
        </p>
        <p>
          LSPs also carry a time to live. This is used to ensure that old
          link-state information is eventually removed from the network. A node
          always decrements the TTL of a newly received LSP before flooding it
          to its neighbors. It also “ages” the LSP while it is stored in the
          node. When the TTL reaches 0, the node refloods the LSP with a TTL of
          0, which is interpreted by all the nodes in the network as a signal to
          delete that LSP.
        </p>
        <h2>Route Calculation</h2>
        <p>
          Once a given node has a copy of the LSP from every other node, it is
          able to compute a complete map for the topology of the network, and
          from this map it is able to decide the best route to each destination.
          The question, then, is exactly how it calculates routes from this
          information. The solution is based on a well-known algorithm from
          graph theory—Dijkstra’s shortest-path algorithm.
        </p>
        <p>
          We first define Dijkstra’s algorithm in graph-theoretic terms. Imagine
          that a node takes all the LSPs it has received and constructs a
          graphical representation of the network, in which N denotes the set of
          nodes in the graph, l(i,j) denotes the nonnegative cost (weight)
          associated with the edge between nodes i, j in N and l(i, j) = ∞ if no
          edge connects i and j. In the following description, we let s in N
          denote this node, that is, the node executing the algorithm to find
          the shortest path to all the other nodes in N. Also, the algorithm
          maintains the following two variables: M denotes the set of nodes
          incorporated so far by the algorithm, and C(n) denotes the cost of the
          path from s to each node n. Given these definitions, the algorithm is
          defined as follows:
        </p>
        <figure>
          <img
            src="images/code-img9.PNG"
            alt="Graph-theoretic implementation of Dijkstra’s algorithm for datagram forwarding"
            class="styled-image"
          />
          <figcaption>
            Figure: Graph-theoretic implementation of Dijkstra’s algorithm for
            datagram forwarding. The node uses link-state packets (LSPs) to
            build a network graph and iteratively computes shortest paths using
            cost metrics.
          </figcaption>
        </figure>
        <p>
          Basically, the algorithm works as follows. We start with M containing
          this node s and then initialize the table of costs (the array
          <em><strong>C(n)</strong></em
          >) to other nodes using the known costs to directly connected nodes.
          We then look for the node that is reachable at the lowest cost (w) and
          add it to M. Finally, we update the table of costs by considering the
          cost of reaching nodes through w. In the last line of the algorithm,
          we choose a new route to node n that goes through node w if the total
          cost of going from the source to w and then following the link from w
          to n is less than the old route we had to n. This procedure is
          repeated until all nodes are incorporated in M.
        </p>
        <p>
          In practice, each switch computes its routing table directly from the
          LSPs it has collected using a realiza tion of Dijkstra’s algorithm
          called the forward search algorithm. Specifically, each switch
          maintains two lists, known as <em><strong>Tentative</strong></em> and
          <em><strong>Confirmed</strong></em
          >. Each of these lists contains a set of entries of the form (<em
            ><strong>Destination</strong></em
          >, <em><strong>Cost</strong></em
          >, <em><strong>NextHop</strong></em
          >). The algorithm works as follows:
        </p>
        <ol>
          <li>
            Initialize the <em><strong>Confirmed</strong></em> list with an
            entry for myself; this entry has a cost of 0.
          </li>
          <li>
            For the node just added to the
            <em><strong>Confirmed</strong></em> list in the previous step, call
            it node <em><strong>Next</strong></em> and select its LSP.
          </li>
          <li>
            For each neighbor (<em><strong>Neighbor</strong></em
            >) of <em><strong>Next</strong></em
            >, calculate the cost (<em><strong>Cost</strong></em
            >) to reach this <em><strong>Neighbor</strong></em> as the sum of
            the cost from myself to <em><strong>Next</strong></em> and from
            <em><strong>Next</strong></em> to <em><strong>Neighbor</strong></em
            >.
          </li>
          <ol>
            <li>
              If Neighbor is currently on neither the
              <em><strong>Confirmed</strong></em> nor the
              <em><strong>Tentative</strong></em> list, then add (<em
                ><strong>Neighbor, Cost, NextHop</strong></em
              >) to the <em><strong>Tentative</strong></em> list, where
              <em><strong>NextHop</strong></em>
              is the direction I go to reach <em><strong>Next</strong></em
              >.
            </li>
            <li>
              If <em><strong>Neighbor</strong></em> is currently on the
              <em><strong>Tentative</strong></em> list, and the Cost is less
              than the currently listed cost for Neighbor, then replace the
              current entry with (<em
                ><strong>Neighbor, Cost, NextHop</strong></em
              >), where <em><strong>NextHop</strong></em> is the direction I go
              to reach <em><strong>Next</strong></em
              >.
            </li>
          </ol>
          <li>
            If the <em><strong>Tentative</strong></em> list is empty, stop.
            Otherwise, pick the entry from the
            <em><strong>Tentative</strong></em> list with the lowest cost, move
            it to the <em><strong>Confirmed</strong></em>
            list, and return to step 2
          </li>
        </ol>
        <figure>
          <img
            src="images/fig3-34.PNG"
            alt="Link-state routing example network"
            class="styled-image"
          />
          <figcaption>
            Figure 3.34: Link-state routing: an example network.
          </figcaption>
        </figure>
        <p>
          This will become a lot easier to understand when we look at an
          example. Consider the network depicted in Figure 3.34. Note that,
          unlike our previous example, this network has a range of different
          edge costs. Table 3.16 traces the steps for building the routing table
          for node D. We denote the two outputs of D by using the names of the
          nodes to which they connect, B and C. Note the way the algorithm seems
          to head off on false leads (like the 11-unit cost path to B that was
          the first addition to the Tentative list) but ends up with the
          least-cost paths to all nodes.
        </p>
        <table aria-labelledby="table-3-16">
          <caption id="table-3-16">
            <strong>Table 3.16:</strong>
            Steps for Building Routing Table for Node D.
          </caption>
          <thead>
            <tr>
              <th>Step</th>
              <th>Confirmed</th>
              <th>Tentative</th>
              <th>Comments</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>(D,0,–)</td>
              <td></td>
              <td>
                Since D is the only new member of the confirmed list, look at
                its LSP.
              </td>
            </tr>
            <tr>
              <td>2</td>
              <td>(D,0,–)</td>
              <td>(B,11,B)<br />(C,2,C)</td>
              <td>
                D’s LSP says we can reach B through B at cost 11, which is
                better than anything else on either list, so put it on Tentative
                list; same for C.
              </td>
            </tr>
            <tr>
              <td>3</td>
              <td>(D,0,–)<br />(C,2,C)</td>
              <td>(B,11,B)</td>
              <td>
                Put lowest-cost member of Tentative (C) onto Confirmed list.
                Next, examine LSP of newly confirmed member (C).
              </td>
            </tr>
            <tr>
              <td>4</td>
              <td>(D,0,–)<br />(C,2,C)</td>
              <td>(B,5,C)<br />(A,12,C)</td>
              <td></td>
            </tr>
            <tr>
              <td>5</td>
              <td>(D,0,–)<br />(C,2,C)</td>
              <td>(B,5,C)<br />(A,12,C)</td>
              <td>
                Cost to reach B through C is 5, so replace (B,11,B). C’s LSP
                tells us that we can reach A at cost 12. Move lowest-cost member
                of Tentative (B) to Confirmed, then look at its LSP.
              </td>
            </tr>
            <tr>
              <td>6</td>
              <td>(D,0,–)<br />(C,2,C)<br />(B,5,C)</td>
              <td>(A,10,C)</td>
              <td>
                Since we can reach A at cost 5 through B, replace the Tentative
                entry.
              </td>
            </tr>
            <tr>
              <td>7</td>
              <td>(D,0,–)<br />(C,2,C)<br />(B,5,C)<br />(A,10,C)</td>
              <td></td>
              <td>
                Move lowest-cost member of Tentative (A) to Confirmed, and we
                are all done.
              </td>
            </tr>
          </tbody>
        </table>
        <p>
          The link-state routing algorithm has many nice properties: It has been
          proven to stabilize quickly, it does not generate much traffic, and it
          responds rapidly to topology changes or node failures. On the
          downside, the amount of information stored at each node (one LSP for
          every other node in the network) can be quite large. This is one of
          the fundamental problems of routing and is an instance of the more
          general problem of scalability. Some solutions to both the specific
          problem (the amount of storage potentially required at each node) and
          the general problem (scalability) will be discussed in the next
          section.
        </p>
        <hr />
        <p><strong>Key Takeaway</strong></p>
        <p>
          Distance-vector and link-state are both distributed routing
          algorithms, but they adopt different strategies. In distance-vector,
          each node talks only to its directly connected neighbors, but it tells
          them everything it has learned (i.e., distance to all nodes). In
          link-state, each node talks to all other nodes, but it tells them only
          what it knows for sure (i.e., only the state of its directly connected
          links). In contrast to both of these algorithms, wewill consider a
          more centralized approach to routing in Section 3.5 when we introduce
          Software Defined Networking (SDN). [Next]
        </p>
        <hr />
        <h2>The Open Shortest Path First Protocol (OSPF)</h2>
        <p>
          One of the most widely used link-state routing protocols is OSPF. The
          first word, “Open,” refers to the fact that it is an open,
          nonproprietary standard, created under the auspices of the Internet
          Engineering Task Force (IETF). The “SPF” part comes from an
          alternative name for link-state routing. OSPF adds quite a number of
          features to the basic link-state algorithm described above, including
          the following:
        </p>
        <ul>
          <li>
            <em>Authentication of routing messages—</em>
            One feature of distributed routing algorithms is that they disperse
            information from one node to many other nodes, and the entire
            network can thus be impacted by bad information from one node. For
            this reason, it’s a good idea to be sure that all the nodes taking
            part in the protocol can be trusted. Authenticating routing messages
            helps achieve this. Early versions of OSPF used a simple 8-byte
            password for authentication. This is not a strong enough form of
            authentication to prevent dedicated malicious users, but it
            alleviates some problems caused by mis configuration or casual
            attacks. (A similar form of authentication was added to RIP in
            version 2.) Strong cryptographic authentication was later added.
          </li>
          <li>
            <em>Additional hierarchy—</em>Hierarchy is one of the fundamental
            tools used to make systems more scalable. OSPF introduces another
            layer of hierarchy into routing by allowing a domain to be
            partitioned into areas. This means that a router within a domain
            does not necessarily need to know how to reach every network within
            that domain—it may be able to get by knowing only how to get to the
            right area. Thus, there is a reduction in the amount of information
            that must be transmitted to and stored in each node.
          </li>
          <li>
            <em>Load balancing—</em>OSPF allows multiple routes to the same
            place to be assigned the same cost and will cause traffic to be
            distributed evenly over those routes, thus making better use of the
            available network capacity.
          </li>
        </ul>
        <figure>
          <img
            src="images/fig3-35.PNG"
            alt="OSPF header format"
            class="styled-image"
          />
          <figcaption>Figure 3.35: OSPF header format.</figcaption>
        </figure>
        <p>
          Figure 3.36 shows the packet format for a type 1 link-state
          advertisement. Type 1 LSAs advertise the cost of links between
          routers. Type 2 LSAs are used to advertise networks to which the
          advertising router is connected, while other types are used to support
          additional hierarchy as described in the next section. Many f ields in
          the LSA should be familiar from the preceding discussion. The LS Age
          is the equivalent of a time to live, except that it counts up and the
          LSA expires when the age reaches a defined maximum value. The Type
          field tells us that this is a type 1 LSA.
        </p>
        <p>
          In a type 1 LSA, the Link state IDandtheAdvertising
          routerfieldareidentical. Each carries a 32-bit identifier for the
          router that created this LSA. While a number of assignment strategies
          may be used to assign this ID, it is essential that it be unique in
          the routing domain and that a given router consistently uses the same
          router ID. One way to pick a router ID that meets these requirements
          would be to pick the lowest IP address among all the IP addresses
          assigned to that router. (Recall that a router may have a different IP
          address on each of its interfaces.)
        </p>
        <p>
          The LS sequence number is used exactly as described above to detect
          old or duplicate LSAs. The LS checksumissimilar to others we have seen
          in other protocols; it is, of course, used to verify that data has not
          been corrupted. It covers all fields in the packet except LS Age, so
          it is not necessary to recompute a checksum every time LS Age is
          incremented. Length is the length in bytes of the complete LSA.
        </p>
        <p>
          Now we get to the actual link-state information. This is made a little
          complicated by the presence of TOS (type of service) information.
          Ignoring that for a moment, each link in the LSA is represented by a
          Link ID, some Link Data, and a metric. The first two of these fields
          identify the link; a common way to do this would be to use the router
          ID of the router at the far end of the link as the Link ID and then
          use the Link Data to disambiguate among multiple parallel links if
          necessary. The metric is of course the cost of the link. Type tells us
          something about the link—for example, if it is a point-to-point link.
        </p>
        <p>
          The TOS information is present to allow OSPF to choose different
          routes for IP packets based on the value in their TOS field. Instead
          of assigning a single metric to a link, it is possible to assign
          different metrics depending on the TOS value of the data. For example,
          if we had a link in our network that was very good for delay-sensitive
          traffic, we could give it a low metric for the TOS value representing
          low delay and a high metric for everything else. OSPF would then pick
          a different shortest path for those packets that had their TOSfield
          set to that value. It is worth noting that, at the time of writing,
          this capability has not been widely deployed.
        </p>
      </section>
    </main>
    <script src="js\theme-toggle.js"></script>
  </body>
</html>
